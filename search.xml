<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[科学上网]]></title>
    <url>%2F2019%2F09%2F25%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[1.概述该篇文档为自己所做笔记，防止后续忘记如何部署。 VPS的购买省略，可以参考该文档《科学上网完全指南》。 附带： 美国VPS Hostwinds IP被屏蔽Ping不通解决新方法 Shadowsocks和V2Ray任选其一 2.Shadowsocks部署2.1 服务器部署 命令行中，输入以下命令，并回车。直接复制即可。这一步是下载安装执行脚本到本地目录下 1wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh 给下载后的脚本添加可执行权限 1chmod +x shadowsocks.sh 执行该脚本，并把标准输出和错误输出放到shadowsocks.log日志中。 输入以下命令执行的过程中，会提示你输入密码、端口和加密方式 1./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 其他常用命令： 启动：/etc/init.d/shadowsocks start 停止：/etc/init.d/shadowsocks stop 重启：/etc/init.d/shadowsocks restart 状态：/etc/init.d/shadowsocks status 2.2 客户端使用参考该文档Shadowsocks的客户端，相当详细了。 3.V2Ray 简单部署3.1 V2Ray服务器部署 下载V2Ray 1wget https://install.direct/go.sh 然后执行脚本安装 V2Ray 123456789101112131415161718192021222324252627282930313233343536373839404142[root@ss-1 ~]# bash go.shInstalling V2Ray v4.19.1 on x86_64Downloading V2Ray: https://github.com/v2ray/v2ray-core/releases/download/v4.19.1/v2ray-linux-64.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 608 0 608 0 0 2026 0 --:--:-- --:--:-- --:--:-- 2033100 11.2M 100 11.2M 0 0 4894k 0 0:00:02 0:00:02 --:--:-- 7245kUpdating software repohttp://mirror.nodesdirect.com/epel/7/x86_64/repodata/d61cfa966bb2988b5ecc41dc76376385f8d0094d473a3d2bfb26c0416adda7eb-updateinfo.xml.bz2: [Errno 14] HTTP Error 404 - Not Found正在尝试其它镜像。To address this issue please refer to the below wiki articlehttps://wiki.centos.org/yum-errorsIf above article doesn't help to resolve this issue please use https://bugs.centos.org/.https://mirror.vcu.edu/pub/gnu%2Blinux/epel/7/x86_64/repodata/ac968a1fb8c66bbce1c8f8e5f524fad2163e3dd799f35a02fd138ed8ae0f6797-filelists.sqlite.bz2: [Errno 14] HTTPS Error 404 - Not Found正在尝试其它镜像。http://mirror.us.leaseweb.net/epel/7/x86_64/repodata/8b3a849ac7aae91918aba9fb66c2d2532e9bb39a2700c8168eff11ad0b6e7b7e-other.sqlite.bz2: [Errno 14] HTTP Error 404 - Not Found正在尝试其它镜像。Installing unzipExtracting V2Ray package to /tmp/v2ray.Archive: /tmp/v2ray/v2ray.zip inflating: /tmp/v2ray/config.json creating: /tmp/v2ray/doc/ inflating: /tmp/v2ray/doc/readme.md inflating: /tmp/v2ray/geoip.dat inflating: /tmp/v2ray/geosite.dat creating: /tmp/v2ray/systemd/ inflating: /tmp/v2ray/systemd/v2ray.service creating: /tmp/v2ray/systemv/ inflating: /tmp/v2ray/systemv/v2ray inflating: /tmp/v2ray/v2ctl extracting: /tmp/v2ray/v2ctl.sig inflating: /tmp/v2ray/v2ray extracting: /tmp/v2ray/v2ray.sig inflating: /tmp/v2ray/vpoint_socks_vmess.json inflating: /tmp/v2ray/vpoint_vmess_freedom.jsonPORT:11234UUID:62b1262c-8c69-4f04-aa33-b45bf1eb8414Created symlink from /etc/systemd/system/multi-user.target.wants/v2ray.service to /etc/systemd/system/v2ray.service.V2Ray v4.19.1 is installed. 以上的 PORT:11234 和 UUID:62b1262c-8c69-4f04-aa33-b45bf1eb8414，在后面会用到。 启动V2Ray 1systemctl start v2ray 开启防火墙 这个步骤一定不要漏，否则后面配置再久也没用。 CentOS 7.0 默认使用的是 firewall 作为防火墙，而不是iptables。 如果你的系统上没有安装，则使用命令安装： 安装firewalld 防火墙 1yum install firewalld 其他常用命令： 开启服务 systemctl start firewalld.service 关闭防火墙 systemctl stop firewalld.service 开机自动启动 systemctl enable firewalld.service 关闭开机制动启动 systemctl disable firewalld.service 开启防火墙上的端口。 – zone #作用域 – add-port=80/tcp #添加端口，格式为：端口/通讯协议 – permanent #永久生效，没有此参数重启后失效 1[root@ss-1 ~]# firewall-cmd --zone=public --add-port=11234/tcp --permanent CentOS 6.0使用iptables规则，放通相应端口 12345vi /etc/sysconfig/iptables-A INPUT -m state --state NEW -m tcp -p tcp --dport 11234 -j ACCEPTsystemctl restart iptables.service #重启防火墙使配置生效 服务器端口的配置文件 /etc/v2ray/config.json。可以从该配置文件修改某些信息 3.2 V2Ray客户端使用神一样的工具们 我在windows中，下载的上面的图形工具是V2RayW。其实解压了该压缩包，运行V2RayW.exe文件也会给出提示要下载V2Ray。 所以下载了V2RayW这个还不够，得需要下载V2Ray。 下载的V2RayW 的压缩包解压后，能看到V2RayW.exe文件。 V2RayW.exe文件需要和v2ray-core文件夹在同一个目录下。v2ray-core文件夹存放V2Ray的文件。如果没有文件夹，则自动手动建立一个。 v2ray-core文件夹内应该包含4个文件v2ray.exe、v2ctl.exe、geosite.dat、geoip.dat 运行该文件V2RayW.exe，在工具栏找到该图标，右键配置。 照下图配置 配置完后，选择加载V2Ray 此时windows主机应该可以正常上网了 附录：给Centos7服务器开BBR加速BBR介绍Google BBR (Bottleneck Bandwidth and RTT) 是一种新的TCP拥塞控制算法,它可以高效增加吞吐和降低网络延迟，并且Linux Kernel4.9+已经集成该算法。开启BBR也非常简单，因为它只需要在发送端开启，网络其他节点和接收端不需要任何改变。 升级内核1. 打开Terminal输入 1# uname -r 查看内核版本，如果输出类似 3.10.0-514.21.2.el7.x86_64 则表示小于4.9，需要升级内核，而如果内核大于等于4.9则跳过至开启Google BBR 2. 升级内核 安装 ELRepo 仓库 12# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 安装最新版kernel 1# yum --enablerepo=elrepo-kernel install kernel-ml -y 确认是否安装成功 1# rpm -qa | grep kernel 如果输出类似如下，包含kernel-ml-4.13.10-1.el7.elrepo.x86_64，则表示安装成功 kernel-3.10.0-693.el7.x86_64kernel-tools-3.10.0-693.el7.x86_64kernel-ml-4.13.10-1.el7.elrepo.x86_64kernel-tools-libs-3.10.0-693.el7.x86_64 设置开机默认启动项 1# egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \' 输出结果类似如下 CentOS Linux 7 Rescue f212d2d7754a4a6bb2b98950c20cc0b5 (4.13.10-1.el7.elrepo.x86_64)CentOS Linux (4.13.10-1.el7.elrepo.x86_64) 7 (Core)CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)CentOS Linux (0-rescue-d1f142097d497f24c021d7de9b81cab4) 7 (Core) 该列表从0开始索引，所以4.13内核索引为1 设置启动项 1# grub2-set-default 1 重启 1# reboot 查看内核版本 1# uname -r 如果输出类似 4.13.10-1.el7.elrepo.x86_64 则表示升级完成 开启Google BBR 修改sysctl配置 123# echo 'net.core.default_qdisc=fq' | tee -a /etc/sysctl.conf# echo 'net.ipv4.tcp_congestion_control=bbr' | tee -a /etc/sysctl.conf# sysctl -p 检查是否加载BBR 1# lsmod | grep bbr 如果输出结果包含tcp_bbr，则表示开启成功 tcp_bbr 20480 0]]></content>
      <categories>
        <category>小技巧</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据包的嗅探与欺骗实验]]></title>
    <url>%2F2019%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%8C%85%E7%9A%84%E5%97%85%E6%8E%A2%E4%B8%8E%E6%AC%BA%E9%AA%97%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[参考文档： 网络工程师的Python之路—Scapy基础篇 网络工程师的Python之路—Scapy应用篇 1.Overview1.概要 Packet sniffing and spoofing are two important concepts in network security; they are two major threats（威胁） in network communication. Being able to understand these two threats is essential（必要） for understanding security measures in networking. There are many packet sniffing and spoofing tools, such as Wireshark, Tcpdump, Netwox, Scapy, etc. Some of these tools are widely used by security experts, as well as by attackers. Being able to use these tools is important for students, but what is more important for students in a network security course is to understand how these tools work, i.e., how packet sniffing and spoofing are implemented in software. 数据包嗅探和欺骗是在网络安全中两个比较重要的概念。它们是在网络通信中两个主要的威胁。能够理解这两种威胁对于理解网络中的安全措施至关重要。有许多数据包监听和嗅探工具，例如Wireshark, Tcpdump, Netwox, Scapy等等。其中的一些工具被安全专家广泛使用，当然也有攻击者。对于学生而言，尽力去使用这些工具很重要，但是对于网络安全课程的学生而言，更重要的是理解这些工具是如何工作的。例如，在软件中数据包的嗅探和欺骗是如何实现的。 The objective of this lab is two-fold: learning to use the tools and understanding the technologies underlying these tools. For the second object, students will write simple sniffer and spoofing programs, and gain an in-depth understanding of the technical aspects（方面） of these programs. This lab covers the following topics: 在这次实验室中目标有两个：学习使用这些工具并且理解这些工具下的技术。对于第二个目标，学生将写一个简单的嗅探和欺骗程序，并且获得对于这些程序在技术方面更为深入的理解。这个实验室覆盖以下主题： Scapy Sniffing using the pcap library Raw socket（原始套接字） Readings and related topics. Detailed coverage of TCP attacks can be found in Chapter 12 of the SEED book, Computer Security: A Hands-on Approach, by Wenliang Du. 阅读材料和相关主题。TCP攻击的相关细节可以在SEED书籍《计算机安全：实践方法，作者杜文亮》的第12章找到。 Lab environment. This lab has been tested on our pre-built Ubuntu 16.04 VM, which can be downloaded from the SEED website. 实验环境。这个实验室已经在我们预先建立好的Ubuntu 16.04 VM中被测试过了，可以在SEED网站下载。 Note for Instructors（讲师）. There are two sets of tasks in this lab. The first set focuses on using tools to conduct（进行，举办） packet sniffing and spoofing. It only requires a little bit of Python programming (usually a few lines of code); students do not need to have a prior（预先） Python programming background. The set of tasks can be used by students with a much broader background. 讲师注意事项。在这个实验中有两个任务。第一个关注于使用工具来进行数据包的嗅探和欺骗。它只需要一点点Python编程（通常就是几行代码）；学生不需要有预先的Python程序背景。这组任务可供具有更广泛背景的学生使用。 The second set of tasks is designed primarily for Computer Science/Engineering students. Students need to write their own C programs from the scratch（从头开始） to do sniffing and spoofing. This way, they can gain a deeper understanding on how sniffing and spoofing tools actually work. Students need to have a solid（固体，扎实） programming background for these tasks. The two sets of tasks are independent; instructors can choose to assign one set or both sets to their students, depending on their students’ programming background. 第二组任务主要是为计算机科学/工程专业的学生设计的。学生需要从头开始写下他们自己的C程序以实现嗅探和欺骗。在这一步，他们可以获得对于嗅探和欺骗工具是如何进行工作更为深入的理解。在这组任务中学生需要有扎实的编程背景。这两组实验是独立的；讲师可以选择分配一组或是两组给学生，这取决于学生的编程背景。 （碎碎念，我太难了。。。。。我选第一组） 该实验拓扑图如下： 2.Lab Task Set 1: Using Tools to Sniff and Spoof Packets2.实验任务组1：使用工具嗅探和欺骗数据包 Many tools can be used to do sniffing and spoofing, but most of them only provide fixed functionalities. Scapy is different: it can be used not only as a tool, but also as a building block（一块积木） to construct other sniffing and spoofing tools, i.e., we can integrate（合并） the Scapy functionalities into our own program. In this set of tasks, we will use Scapy for each task. 许多工具能够被用来嗅探和欺骗，但是其中大部分只能提供固定功能。Scapy则不同：它不仅可以被用作一个工具，还可以作为一个构建其他嗅探和欺骗工具的基石。例如，我们能把Scapy的功能加到我们的程序中。在这组任务中，我们将使用Scapy完成每一个任务。 To use Scapy, we can write a Python program, and then execute this program using Python. See the following example. We should run Python using the root privilege because the privilege is required for spoofing packets. At the beginning of the program (Line º), we should import all Scapy’s modules. 为了使用Scapy，我们将写一个Python程序，然后使用Python执行这个程序。参考以下例子。我们应该使用root用户权限运行Python，因为对于嗅探数据包需要该权限。在程序开始处1，我们将导入所有的Scapy模块。 123456789101112$ view mycode.py #!/bin/bin/pythonfrom scapy.all import * //1a = IP()a.show()$ sudo python mycode.py ###[ IP ]###version = 4ihl = None... We can also get into the interactive mode of Python and then run our program one line at a time at the Python prompt. This is more convenient if we need to change our code frequently in an experiment. 我们还可以进入python的交互模式，然后在python提示符上，一次只运行一行程序。如果我们需要在实验中频繁地更改代码，这就更方便了。 12345678$ sudo python&gt;&gt;&gt; from scapy.all import *&gt;&gt;&gt; a = IP()&gt;&gt;&gt; a.show() ###[ IP ]###version = 4ihl = None... 2.1 Task 1.1: Sniffing Packets任务1.1嗅探数据包 Wireshark is the most popular sniffing tool, and it is easy to use. We will use it throughout the entire lab. However, it is difficult to use Wireshark as a building block to construct other tools. We will use Scapy for that purpose. The objective of this task is to learn how to use Scapy to do packet sniffing in Python programs. A sample code is provided in the following: Wireshark是最受欢迎的嗅探工具，并且它易于使用。我们将使用它贯穿整个实验。然而，我们却很难使用Wireshark去作为一个基础工具来构建其他工具。我们将使用Scapy来达到这一目的。这个任务的目标是学习如何在Python程序中使用Scapy来实现数据包嗅探。以下提供了一个简单的代码： 1234567#!/usr/bin/pythonfrom scapy.all import *def print_pkt(pkt): pkt.show()pkt = sniff(filter=’icmp’,prn=print_pkt) Task 1.1A.The above program sniffs packets. For each captured packet, the callback function print pkt() will be invoked（被引用）; this function will print out some of the information about the packet. Run the program with the root privilege and demonstrate（证明） that you can indeed（语气词，强调） capture packets. After that, run the program again, but without using the root privilege; describe and explain your observations. 以上程序嗅探数据包。对于每个被捕获的包，将调用回调函数打印（callback function print）PKT()；此函数将打印出关于数据包的一些信息。使用root权限运行程序，并证明你确实可以捕获数据包。在此之后，再次运行程序，但不使用root权限；描述和解释你的观察结果。 1.我在拓扑图中的第一台虚拟机下的当前用户目录下，创建了一个zhangshuaiyang的目录 2.然后把上述程序内容，复制到了zhangshuaiyang目录下的task1.1.py文本中 3.修改文件权限为764 4.使用seed用户运行该程序，可以看到如下报错PermissionError: [Errno 1] Operation not permitted。该报错的解决方案是：This means that you need to start your script with sudo/admin rights.也即执行第5步。 5.此时使用root权限再执行该脚本，能正常执行。同时我在第二台虚拟机上ping第一台虚拟机的ip，发现有数据包经过。抓包内容如表中所示，其实很容易看出来，抓的包有2个，分别是icmp的请求包和回应包。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&gt; [09/23/19]seed@VM:~/zhangshuaiyang$ sudo ./task1.1.py&gt; ###[ Ethernet ]### &gt; dst = 08:00:27:04:d9:58&gt; src = 08:00:27:0f:63:a6&gt; type = 0x800&gt; ###[ IP ]### &gt; version = 4&gt; ihl = 5&gt; tos = 0x0&gt; len = 84&gt; id = 63331&gt; flags = DF&gt; frag = 0&gt; ttl = 64&gt; proto = icmp&gt; chksum = 0x2b33&gt; src = 10.0.2.4&gt; dst = 10.0.2.15&gt; \options \&gt; ###[ ICMP ]### &gt; type = echo-request&gt; code = 0&gt; chksum = 0x5609&gt; id = 0xb71&gt; seq = 0x1&gt; ###[ Raw ]### &gt; load = '\x8c\xe2\x88]\x8dA\t\x00\x08\t\n\x0b\x0c\r\x0e\x0f\x10\x11\x12\x13\x14\x15\x16\x17\x18\x19\x1a\x1b\x1c\x1d\x1e\x1f !"#$%&amp;\'()*+,-./01234567'&gt; &gt; ###[ Ethernet ]### &gt; dst = 08:00:27:0f:63:a6&gt; src = 08:00:27:04:d9:58&gt; type = 0x800&gt; ###[ IP ]### &gt; version = 4&gt; ihl = 5&gt; tos = 0x0&gt; len = 84&gt; id = 42397&gt; flags = &gt; frag = 0&gt; ttl = 64&gt; proto = icmp&gt; chksum = 0xbcf9&gt; src = 10.0.2.15&gt; dst = 10.0.2.4&gt; \options \&gt; ###[ ICMP ]### &gt; type = echo-reply&gt; code = 0&gt; chksum = 0x5e09&gt; id = 0xb71&gt; seq = 0x1&gt; ###[ Raw ]### &gt; load = '\x8c\xe2\x88]\x8dA\t\x00\x08\t\n\x0b\x0c\r\x0e\x0f\x10\x11\x12\x13\x14\x15\x16\x17\x18\x19\x1a\x1b\x1c\x1d\x1e\x1f !"#$%&amp;\'()*+,-./01234567'&gt; Task 1.1B.Usually, when we sniff packets, we are only interested certain types of packets. We can do that by setting filters in sniffing. Scapy’s filter use the BPF (Berkeley Packet Filter) syntax; you can find the BPF manual from the Internet. Please set the following filters and demonstrate your sniffer program again (each filter should be set separately（单独地）): 通常，当我们嗅探数据包时，我们只对某些类型的数据包感兴趣。我们可以通过设置过滤器进行嗅探。Scapy的过滤器使用bpf（伯克利包过滤器）语法；你可以从互联网上找到bpf手册。请设置以下过滤器并再次证明你的嗅探器程序（每个过滤器应分别设置）： Capture only the ICMP packet 仅捕获ICMP数据包 Capture any TCP packet that comes from a particular IP and with a destination port number 23. 捕获特定IP的任意TCP数据包，并且要求目的端口是23 Capture packets comes from or to go to a particular subnet. You can pick any subnet, such as 128.230.0.0/16; you should not pick the subnet that your VM is attached to. 捕获来自或发往特定子网的数据包。你能选择任意子网，例如123.230.0.0/16；你不应该选择你的VM所连接的子网 2.2 Task 1.2: Spoofing ICMP Packets任务1.2 欺骗ICMP数据包 As a packet spoofing tool, Scapy allows us to set the fields of IP packets to arbitrary（任意的） values. The objective of this task is to spoof IP packets with an arbitrary source IP address. We will spoof ICMP echo request packets, and send them to another VM on the same network. We will use Wireshark to observe whether our request will be accepted by the receiver. If it is accepted, an echo reply packet will be sent to the spoofed IP address. The following code shows an example of how to spoof an ICMP packets. 作为一个包欺骗工具，Scapy允许我们去设置IP数据包字段的任意值。这个任务的目的是欺骗带有任意源IP地址的IP数据包。我们将欺骗ICMP的echo请求数据包，并且把他们发送到处于相同网络的另一台VM上。我们将使用Wireshark来验证我们的请求是否会被另外一台接收者主机所接受。如果接收方接收到了，一个echo应答包将被发送到被欺骗的IP地址上。以下代码展示了一个案例，即如何欺骗一个ICMP数据包。 12345678&gt;&gt;&gt; from scapy.all import *&gt;&gt;&gt; a = IP() //1&gt;&gt;&gt; a.dst = ‘10.0.2.3’ //2&gt;&gt;&gt; b = ICMP() //3&gt;&gt;&gt; p = a/b //4&gt;&gt;&gt; send(p) //5.Sent 1 packets. In the code above, Line1 creates an IP object from the IP class; a class attribute（属性） is defined for each IP header field. We can use ls(a) or ls(IP) to see all the attribute names/values. We can also use a.show() and IP.show() to do the same. Line2 shows how to set the destination IP address field. If a field is not set, a default value will be used. 在以上代码中，行1从IP类中创建了一个IP对象；为每个IP头部字段定义一个类属性。我们能使用ls(a)或是ls(IP)来查看所有的属性名称/数值。我们也可以使用 a.show()和IP.show()来做同样的事情。行2展示了如何设置字段中的目的IP地址。如果一个字段没有设置，那将会使用默认的数值。 1234567891011121314&gt;&gt;&gt; ls(a)version : BitField (4 bits) = 4 (4)ihl : BitField (4 bits) = None (None)tos : XByteField = 0 (0)len : ShortField = None (None)id : ShortField = 1 (1)flags : FlagsField (3 bits) = &lt;Flag 0 ()&gt; (&lt;Flag 0 ()&gt;)frag : BitField (13 bits) = 0 (0)ttl : ByteField = 64 (64)proto : ByteEnumField = 0 (0)chksum : XShortField = None (None)src : SourceIPField = '10.0.2.4' (None)dst : DestIPField = '10.0.2.3' (None)options : PacketListField = [] ([]) Line 3 creates an ICMP object. The default type is echo request. In Line 4, we stack a and b together to form a new object. The / operator is overloaded by the IP class, so it no longer represents（代表） division（除法）; instead, it means adding b as the payload field of a and modifying（修改） the fields of a accordingly（从而，于是）. As a result, we get a new object that represent an ICMP packet. We can now send out this packet using send() in Line 5. Please make any necessary change to the sample code, and then demonstrate（证明） that you can spoof an ICMP echo request packet with an arbitrary source IP address. 行3创建了一个ICMP对象。默认类型是echo请求。在行4，我们把a和b堆叠在了一起以创建一个新的对象。/操作是被IP类重载，因此它将不在表示除法；相反，他意味着添加b作为a的负载字段，并且修改a的字段。（注：在这里的意思就是经常听到的，ICMP被IP封装。）结果就是，我们得到一个新的对象来代表一个ICMP数据包。在行5，我们现在就可以把这个数据包使用send()发送出去了。请对这段简单代码做出任何有必要的改变，然后证明你能欺骗一个带有任意源IP的ICMP请求数据包 2.3 Task 1.3: Traceroute任务1.3 路由追踪 The objective of this task is to use Scapy to estimate（预估） the distance, in terms of number of routers, between your VM and a selected destination. This is basically what is implemented by the traceroute tool. In this task, we will write our own tool. The idea is quite straightforward: just send an packet (any type) to the destination, with its Time-To-Live (TTL) field set to 1 first. This packet will be dropped by the first router, which will send us an ICMP error message, telling us that the time-to-live has exceeded. That is how we get the IP address of the first router. We then increase our TTL field to 2, send out another packet, and get the IP address of the second router. We will repeat this procedure（程序，步骤） until our packet finally reach the destination. It should be noted that this experiment only gets an estimated result, because in theory（理论）, not all these packets take the same route (but in practice（练习）, they may within a short period of time). The code in the following shows one round in the procedure. 这个实验的目的是使用Scapy来预估在我们的VM和被选择的目的地之间的路径（以路由器的数量为单位）。这基本是通过路由追踪工具实现的。在这个任务中，我们将写一个我们自己的工具。这个想法相当简单直白：仅发送一个数据包（任意类型）到达目的地，并首先设置它的TTL字段为1。这个包将会被第一个路由器所丢弃，路由器则发送给我们一个ICMP错误信息。我们接下来增加我们的TTL字段为2，发送另一个数据包，则可以获得第二个路由器的IP地址。我们将重复这些步骤，直到我们的数据包最终到达目的地。需要注意的是，这个实验仅能获得一个预估的结果，因为理论上来说，不是所有的数据包都会经过同一个路由器（但是在练习中，他们可能会在很短的时间段内）。以下代码展示了在该过程中的一轮操作。 12345a = IP()a.dst = ’1.2.3.4’a.ttl = 3 b = ICMP()send(a/b) If you are an experienced Python programmer, you can write your tool to perform the entire procedure automatically. If you are new to Python programming, you can do it by manually changing the TTL field in each round, and record the IP address based on your observation（意见，注目） from Wireshark. Either way is acceptable, as long as you get the result. 如果你是一个经验丰富的Python程序员，你能写下你自己的工具来自动执行整个过程。如果你是一个Python新手，则可以通过在每个回合中手动更改TTL字段来实现，并根据Wireshark的观察记录IP地址。两种方式都是可以接受的，只要你获得了结果。 2.4 Task 1.4: Sniffing and-then Spoofing任务1.4 嗅探然后欺骗 In this task, you will combine the sniffing and spoofing techniques to implement the following sniff-and- then-spoof program. You need two VMs on the same LAN. From VM A, you ping an IP X. This will generate（产生） an ICMP echo request packet. If X is alive, the ping program will receive an echo reply, and print out the response. Your sniff-and-then-spoof program runs on VM B, which monitors（监控） the LAN through packet sniffing. Whenever it sees an ICMP echo request, regardless（无论） of what the target IP address is, your program should immediately send out an echo reply using the packet spoofing technique. Therefore, regardless of whether machine X is alive or not, the ping program will always receive a reply, indicating that X is alive. You need to use Scapy to do this task. In your report, you need to provide evidence（证据） to demonstrate that your technique works. 在这个任务中，你将结合嗅探和欺骗技术来实现以下的嗅探然后欺骗程序。你需要在同一个局域网中有2个VM。从VM A，你ping一个IP X。这将产生一个ICMPecho请求数据包。如果X是活跃状态，ping程序将会接收一个echo回应，并且打印出回应结果。你的嗅探然后欺骗程序运行在VM B，它监控这个局域网通过的数据包。每当它看到ICMP echo请求时，无论目标IP地址是什么，你的程序都应立即使用数据包欺骗技术发出回显应答。因此，无论机器X是否处于活动状态，ping程序都将始终收到答复，表明X处于活动状态。你需要使用Scapy来完成该任务。在你的报告中，你需要提供证据以证明你的技术有效。]]></content>
      <categories>
        <category>SEED Labs</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何使用VirtualBox运行SEED_Ubuntu_VM？]]></title>
    <url>%2F2019%2F09%2F07%2F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VirtualBox%E8%BF%90%E8%A1%8CSEED_Ubuntu_VM%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[官方原文档 Install the free VirtualBox software first. We recommend Version 6.0.4 (please stay away from the newer versions, as they still have some issues with our VM). 首先安装免费的VirtualBox软件。 我们建议使用版本6.0.4（请远离新版本，因为它们仍然存在我们的VM的一些问题）。 Step 1: Create a New VM in VirtualBox步骤1：在VirtualBox中创建一个新的VM Step 2: Provide a Name and Select the OS Type and Version步骤2：进行命名，同时选择操作系统的类型与版本 Do NOT pick Ubuntu (64-bit), even though your machine is 64 bit. Our prebuilt VM is 32-bit Ubuntu. 不要选择Ubuntu (64-bit)，尽管你的机器是64位的。我们预建的VM是32位的Ubuntu。 Step 3: Set the Memory Size步骤3：设置内存容量 Step 4: Select the Pre-built VM File Provided by Us步骤4： 选择使用我们提供的预建的VM文件 In the above step, you may encounter(遭遇) the following error; otherwise, directly go to Step 5.在上面的步骤中，你可能会遇到以下错误; 如果没有，则直接转到步骤5。 Reason and Solution: This is because you copied the VM files from another VM, which is already loaded into VirtualBox. These two VMs have the same UUID, which is not allowed by Virtualbox. Here are several solutions depending on your situations: 原因和解决方法：这是因为你从其他VM复制VM文件，而这个VM已经被加载过VirtualBox中了。这两个VM有着同样的UUID，而在VirtualBox中是不允许这样的。取决于你的状况，有以下几个解决方案： If you plan to create multiple VMs using the same image, please use the clonemechanism (See Appendix A for details). 如果你使用相同的镜像创建多个VM，请使用克隆机制（更多细节请参考附录A） If the older VM with the same UUID is no longer needed, remove it from VirtualBox will solve the problem. 如果不再需要具有相同UUID的旧的VM，则从VirtualBox中删除它可以解决问题。 If you do want to keep the older VM, you can change the UUID of the new VM. The fastest way is to directly modify SEEDUbuntu16.04.vmdk, which is a text file. Search for the ddb.uuid.image entry, and change its value (e.g., change the last byte from ‘d’ to ‘e’) 如果你确实想要保留旧的VM，你可以为新的VM修改UUID。 最快的方法是直接修改SEEDUbuntu16.04.vmdk，这是一个文本文件。 搜索ddb.uuid.image条目，并更改其值（例如，将最后一个字节从“d”更改为“e”） If there is no error (or after you fix the error), your VM will be created successfully. 如果没有出现错误（或是你已经修复了错误），你的VM将成功创建。 Step 5: Configure the VM步骤5：配置VM Step 6: Start the VM步骤6：启动VM Step 7: Stop the VM or Save the VM’s State步骤7：停止VM或是保存VM的状态 When you are done with your VM, you can always shut it down (from inside Ubuntu). A better alternative is to “freeze” the computer, so everything is saved. When you need it again, you can “unfreeze” it, and resume from where you left off. This is much faster and convenient than shutting down and rebooting the VM. To achieve this, you can use the “Save State” option. 用完VM后，你可以随时关闭它（从Ubuntu内部）。 更好的选择是“freeze冻结”计算机，以便保存所有内容。 当你再次需要它时，你可以“unfreeze解冻”它，并从你离开的地方恢复。 这比关闭和重新启动VM快得多，方便快捷。 为此，你可以使用“Save State保存状态”选项。 Appendix A: Use “Clone” to create Multiple VMs附录A：使用“克隆”以创建多个VM Some SEED labs require multiple VMs. The easiest way to create multiple VMs is to create one first, and then use the “Clone” mechanism to clone it. Before doing the cloning, please ensure the following: 一些SEED实验需要多个VM。 创建多个VM的最简单方法是首先创建一个，然后使用“克隆”机制来克隆它。 在进行克隆之前，请确保以下内容： IMPORTANT: make sure that the VM is fully shutdown (not in a “Saved” state), or there will be all sorts of problems. 重要：确保你的VM已经完全关闭（而不是处于“保存状态”），不然可能会有各种各样的问题。 Configure network (see Appendix B); otherwise you have to do it for each VM. 配置网络（参考附录B）；不然你就得为每个VM配置它了。 Configure folder sharing (see Appendix D); otherwiseyou have to do it for each VM. 配置文件夹共享（参考附录B）；不然你就得为每个VM配置它了。 The clone will take a few minutes, depending on the speed of your computer. 克隆将会持续几分钟，这取决于你的电脑。 Appendix B: Network Configuration in VirtualBox for SEED Labs附录B：为SEED实验在VirtualBox上配置网络 In many of the SEED labs, we need to run multiple guest VMs, and these VMs should be able to (1) reach out to the Internet, (2) communicate with each other. In Virtualbox, if we use the “NAT” setting (default setting) for each VM, we can achieve 1, but not 2, because each VM will be placed in its own private network, not on a common one; they even have the same IP address, which is not a problem because each VM is the only computer on its own private network. On the other hand, if we use the “Host-only” setting for each VM, we can achieve 2, but not 1. Using this setting, all the VMs and the host will be put on a common network, so they can communicate with each other; however, due to the lack of NAT, the VMs cannot reach out to the outside. 在许多SEED实验中，我们需要运行多个客户虚拟机，这些虚拟机应该能够（1）接入互联网，（2）相互通信。在Virtualbox中，如果我们为每个VM使用“NAT”设置（默认设置），我们可以实现1而不是2，因为每个VM将放置在自己的专用网络中，而不是普通的网络中;它们甚至具有相同的IP地址，这不是问题，因为每个VM是其自己的专用网络上唯一的计算机。另一方面，如果我们为每个VM使用“仅主机”的设置，我们可以实现2但不能达到1.使用此设置，所有VM和主机将被放在一个公共网络上，因此它们可以进行通信彼此;但是，由于缺少NAT，虚拟机无法与外界联系。 Therefore, in order to achieve all these 2 goals, we have to use a network adapter called “NAT Network”. The adapter works in a similar way to “local area network” or LAN. It enable VMs communication within same local network as well as the communication to the internet. All the communication goes through this single adapter. As show in Figure 1, gateway router transfers the packets among the VMs and transfers the packets from local network to Internet. 因此，为了同时实现这两个目标，我们必须使用称为“NAT网络”的网络适配器。适配器的工作方式与“局域网”或LAN类似。它支持在同一本地网络内进行VM通信以及与Internet的通信。所有通信都通过这个单一适配器。如图1所示，网关路由器在VM之间传输数据包，并将数据包从本地网络传输到Internet。 Configuration Instruction配置介绍 Step 1: Make sure you are using the most up-to-date VirtualBox. As show in the following figure, click the “File” on the top left of the VirtualBox main UI. Then choose “Preferences…” option. 步骤1：确保您使用的是最新的VirtualBox。 如下图所示，单击VirtualBox主UI左上角的“文件”。 然后选择“首选项…”选项。 Step 2: Click the “Network” tab on left panel. click the “+” button to create a new NAT Networks (NatNetwork) adaptor (if one does not exist). Double click on the NatNetwork, and look at its specifications. Set the specifications as the same as what is shown below. 步骤2：点击左侧栏中的“网络”键。点击“+”号按钮以创建一个新的NAT网络适配器（如果一个都没有的话）。双击这个NatNetwork，并且查看它的格式规范，照下图的配置来进行设置。 Step 4: Go to VM setting, you need to power off the VM before making the following changes. Enable Adapter 1(at the same time, disable the other adapters), and choose “NAT Network”. 步骤4：回到VM的设置，你需要确保在做以下操作的前已经对VM完全关机。开启适配器1（与此同时关闭其他适配器），并且选择“NAT网络”。 Step 5: Now power on the VM, and check the IP address. 步骤5：现在开启VM，并且检查IP地址 Troubleshooting: If VMs can not ping each other, refresh the MAC Address can resolve the issue. The way to resolve the issue is shown in figure 4, troubleshoot 1. 如果VM之间无法互相ping通，重新获取MAC地址能解决该问题。解决问题的方法在上面步骤4，点击那个 troubleshoot 1。 Appendix C: Take Snapshots and Recover from Snapshots附录C：创建快照与从快照中恢复 For some labs, you may need to make changes to the operating system. If you make a severe mistake, your VM may not be able to boot up again, and you will lost everything inside the failed VM. have done. To avoid such trouble, before doing anything dangerous to the OS, it is better to take a snapshot of your current VM. You can take as many snapshots as you want. 对于一些实验，你可能需要对操作系统做出改变。如果你的操作导致服务器出现问题，你的VM可能无法再次启动了，并且你将丢失在这个故障VM中的所有内容。在对操作系统做任何危险操作前，为了避免上述问题出现，最好就是对当前的VM创建快照。只要你想，你能创建任意个数量的快照。 To restore from a snapshot that you have taken before, you can click the followings (you need to shut-down the VM first): 要从之前拍摄的快照还原，可以单击以下内容（你需要先关闭VM）： Appendix D: Folder Sharing附录D：文件夹分享 Files can be shared between the host computer and the guest operating system in VirtualBox. The following steps show how to do so. 在主机电脑和VirtualBox中的客户操作系统间，文件是可以共享的。以下步骤将告诉你如何去做。 Create the folder to be shared on the host computer. In this tutorial we name the folder share. 在主机电脑上创建一个用于共享的文件夹。在本教程中，我们给文件夹取名叫share。 Boot the Guest operating system in VirtualBox. 在VirtualBox中启动用户操作系统。 Go to the Settings popup window, and select “Shared Folders” 回到设置弹出窗口，并选择“分享文件夹”。 Choose the ‘Add’ button. 选择“添加”按钮。 Choose “Other …”, and select a folder from the popup window. 选择“其他……”，并从弹出窗口中选择文件夹。 Select Auto Mount and Make Permanent option. Click OK. Click OK again to close the Settings Dialog. 选择动态挂载和Make Permanent（常设，永久） 选项。点击完成。再次点击完成以关闭设置对话框。 Open a terminal in the VM. Make a directory and name it host (you can choose any name you like). Use command “mkdir /home/seed/host” 打开VM中的终端，创建一个目录并将其命名为host（你可以选择任何您喜欢的名称）。使用命令“mkdir /home/seed/host”（因为上面那张图的挂载目录写的就是这个文件夹地址，所以你也得确保在VM中创建了这个文件夹） We want files in our mount point (~/host) to be owned by the current user. Also we want the mounted shared folder to persist after reboot. Hence, we will edit the /etc/rc.local file (using “sudo gedit /etc/rc.local”) and add the command below (1000 is the User ID and group ID of the user seed): 我们希望挂载点（〜/ host）中的文件归当前用户所有。 此外，我们希望挂载的共享文件夹在重新启动后保持不变。 因此，我们将编辑 /etc/rc.local文件（使用“sudo gedit /etc/rc.local”）并添加以下命令（1000是用户ID和用户种子的组ID）： sudo mount -t vboxsf -o rw,uid=1000,gid=1000 share /home/seed/host Save the changes and reboot VM. Now anything placed in /home/seed/host inside the VM should be visible from the share folder on the host machine, and vice versa. 保存更改并重启VM。 现在，从主机上的共享文件夹中可以看到放在VM中/home/seed/host中的任何内容，反之亦然。]]></content>
      <categories>
        <category>SEED Labs</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2019%2F08%2F11%2Fmarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[插入数学公式在Markdown中插入数学公式的语法是$数学公式$和$$数学公式$$。 行内公式是可以让公式在文中与文字或其他东西混编，不独占一行。 示例 1质能方程$E = mc^2$ 显示 质能方程$E=mc^2$ 独立公式使公式单独占一行，不与文中其他文字等混编。 示例 1质能方程$$E = mc^2$$ 显示 质能方程 $$E=mc^2$$ 普通公式普通的加减乘除数学公式的输入方法与平常的书写一样。 示例 1$$x = 100 * y + z - 10 / 33 + 10$$ 显示 $$x=100∗y+z−10/33+10$$ 上下标使用^来表示上标，_来表示下标，同时如果上下标的内容多于一个字符，可以使用{}来将这些内容括起来当做一个整体。与此同时，上下标是可以嵌套的。 示例 1$$x = a_&#123;1&#125;^n + a_&#123;2&#125;^n + a_&#123;3&#125;^n$$ 显示 $$x = a_{1}^n + a_{2}^n + a_{3}^n$$ 如果希望左右两边都能有上下标，可以使用\sideset语法 示例 1$$\sideset&#123;^1_2&#125;&#123;^3_4&#125;A$$ 显示 $$\sideset{^1_2}{^3_4}A$$ 括号()，[]和|都表示它们自己，但是{}因为有特殊作用因此当需要显示大括号时一般使用\lbrace \rbrace来表示。 示例 1$$f(x, y) = 100 * \lbrace[(x + y) * 3] - 5\rbrace$$ 显示 $$f(x, y) = 100 * \lbrace[(x + y) * 3] - 5\rbrace$$ 分数分数使用\frac{分母}{分子}这样的语法，不过推荐使用\cfrac来代替\frac，显示公式不会太挤。 示例 1$$\frac&#123;1&#125;&#123;3&#125; 与 \cfrac&#123;1&#125;&#123;3&#125;$$ 显示 $$\frac{1}{3} 与 \cfrac{1}{3}$$ 开方开方使用\sqrt[次数]{被开方数}这样的语法 示例 12$$\sqrt[3]&#123;X&#125;$$$$\sqrt&#123;5 - x&#125;$$ 显示 $$\sqrt[3]{X}$$$$\sqrt{5 - x}$$ 希腊字母见下表 代码 大写 代码 小写 A A \alpha α B B \beta β \Gamma Γ \gamma γ \Delta Δ \delta δ E E \epsilon ϵ Z Z \zeta ζ H H \eta η \Theta Θ \theta θ I I \iota ι K K \kappa κ \Lambda Λ \lambda λ M M \mu μ N N \nu ν \Xi Ξ \xi ξ O O \omicron ο \Pi Π \pi π P P \rho ρ \Sigma Σ \sigma σ T T \tau τ \Upsilon Υ \upsilon υ \Phi Φ \phi ϕ X X \chi χ \Psi Ψ \psi ψ \Omega Ω \omega ω 其他字符关系运算符 符号 代码 ± \pm × \times ÷ \div ∣ \mid ∤ \nmid ⋅ \cdot ∘ \circ ∗ \ast ⨀ \bigodot ⨂ \bigotimes ⨁ \bigoplus ≤ \leq ≥ \geq ≠ \neq ≈ \approx ≡ \equiv ∑ \sum ∏ \prod ∐ \coprod 集合运算符 符号 代码 ∅ \emptyset ∈ \in ∉ \notin ⊂ \subset ⊃ \supset ⊆ \subseteq ⊇ \supseteq ⋂ \bigcap ⋃ \bigcup ⋁ \bigvee ⋀ \bigwedge ⨄ \biguplus ⨆ \bigsqcup 对数运算符 符号 代码 log \log lg \lg ln \ln 三角运算符 符号 代码 ⊥ \bot ∠ \angle sin \sin cos \cos tan \tan cot \cot sec \sec csc \csc 微积分运算符 符号 代码 ′ \prime ∫ \int ∬ \iint ∭ \iiint ∬∬ \iiiint ∮ \oint lim \lim ∞ \infty ∇ \nabla d \mathrm{d}]]></content>
      <categories>
        <category>小技巧</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[grep/正则表达式/awk/sed]]></title>
    <url>%2F2019%2F08%2F11%2Fgrep-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-awk-sed%2F</url>
    <content type="text"><![CDATA[Linux三剑客 grep适合单纯的查找或是匹配文本 sed适合编辑我们查找到的文本 awk对于查找到的文本，可以进行格式化处理，即输出成我们想要看到的样子 正则表达式是公式。我们可以在三剑客中按（正则表达式）条件查找到我们需要的东西 1.grepgrep（global search regular expression and print out the line） grep “需要搜索的字符” grep -i “需要搜索的字符” 不区分大小写 grep -n “需要搜索的字符” 在行首显示字符所在行号 grep –color=auto “需要搜索的字符” 字符用颜色显示 grep –color “需要搜索的字符” 字符用颜色显示 grep -c “需要搜索的字符” 显示字符一共所在多少行 grep -o “需要搜索的字符” 显示字符，在单独的一行输出 grep -Bx “需要搜索的字符” 显示字符所在的行及其前x行 grep -Ax “需要搜索的字符” 显示字符所在的行及其后x行 grep -Cx “需要搜索的字符” 显示字符所在的行及其前后x行 grep -w “需要搜索的字符” 精确显示字符所在的行，比如字符是abc，就不会显示abcd所在行 grep -v “需要搜索的字符” 显示不包含该字符所在的行 grep -e “需要搜索的字符” -e “需要搜索的字符2” 显示多个目标字符所在行 2.正则表达式2.1基本正则表达式grep “^hello” 打印出以hello开头的行 grep “hello$” 打印出以hello结尾的行 grep “^hello$” 打印出只有hello单词的行 grep “^$” 打印出空行，该行中有空格也不能算是一个空行 grep “\ &lt; hello” 打印出以hello作为一个单词词首的一行 grep “hello\ &gt;” 打印出以hello作为一个单词词尾的一行 grep “\ &lt; hello\ &gt;” 打印出以hello作为一个单词的一行 同时可以使用\ b来表示锚定词首或词尾，即来替代\ &gt;和\ &lt; 使用\ B可以表示词首或词尾不是某个单词 连续次数的匹配grep “a\ {3\ }” 打印出单词中有3个连续a的一行 grep “\ &lt; a\ {3\ } \ &gt;” 打印出单词中只有3个连续a的一行（以连续3个a作为一个单词） grep “a\ {2,4\ } “ 打印出单词中至少有连续2个a或最多连续4个a的一行 grep “a\ { ,4\ } “ 打印出单词中最多连续4个a的一行 grep “a\ {2,\ } “ 打印出单词中至少连续2个a的一行 grep “a*” 打印出单词中有n个a的一行 grep “a.*” 打印出单词中有a且a后面接上任意个数目及字符的行 grep “a\ ?” 打印出单词中能匹配到0个或1个a的行（其实有2个以上的a也能匹配到，因为2个以上的a包含0个或1个a） grep “a\ +” 打印出单词中能匹配到至少1个a的行 常用符号grep “a…” 打印出单词中能匹配到a后面接上3个任意字符的行 grep “a[[:alpha:]]\ { 3\ }” 打印出单词中能匹配到a后面接上了3个任意大小写字母的行 [[:alpha:]] 表示任意大小写字母[[:lower:]] 表示任意小写字母[[:upper:]] 表示任意大写字母[[:digit:]] 表示0到9之间的任意单个数字（包括0和9）[[:alnum:]] 表示任意数字或字母[[:space:]] 表示任意空白字符，包括”空格”、”tab键”等。[[:punct:]] 表示任意标点符号 [0-9]与[[:digit:]]等效[a-z]与[[:lower:]]等效[A-Z]与[[:upper:]]等效[a-zA-Z]与[[:alpha:]]等效[a-zA-Z0-9]与[[:alnum:]]等效 grep “a[ ^[:alpha:]]\ { 3\ }” 打印出单词中能匹配到a后面接上了3个不是大小写字母的行 [^ 0-9]与[ ^[:digit:]]等效[ ^ a-z]与[ ^[:lower:]]等效[ ^ A-Z]与[ ^[:upper:]]等效[ ^ a-zA-Z]与[ ^[:alpha:]]等效[ ^ a-zA-Z0-9]与[ ^[:alnum:]]等效 grep “a[efg]” 打印出单词中能匹配到a后面接上了e或f或g的行 grep “a[ ^efg]” 打印出单词中能匹配到a后面接上不是e或f或g的行 同时还有简短格式，但是并非所有正则表达式解析器都可以识别。使用的时候可以尝试加上-P参数\d 表示任意单个0到9的数字\D 表示任意单个非数字字符\t 表示匹配单个横向制表符（相当于一个tab键）\s表示匹配单个空白字符，包括”空格”，”tab制表符”等\S表示匹配单个非空白字符 分组及向后引用grep “\ (hello\ ) \ {2\ }” 以hello为一个组（第一个分组），打印出单词中能匹配到2次hello的行 grep “\ (hello\ ) word \1” 以hello为一个组，打印出有hello word hello的行。其中\1表示第一个分组 转义符grep “a\ .\ .” 打印出单词中能匹配到a..的行 grep “a\ \ *” 打印出单词中能匹配到a*的行 grep ‘a\ \ \ \ ‘ 打印出单词中能匹配到a\ \的行，注意此时使用单引号 2.2扩展正则表达式使用扩展正则表达式时，要加上-E参数 在扩展正则表达式中，有|这个符号，按住“shift键”和“\键”就可以打出 grep -E “(com|net)$” 打印出以com或net结尾的行 在基本正则表达式中需要加\，但是扩展中不需要的如下： ? 表示匹配其前面的字符0或1次 + 表示匹配其前面的字符至少1次，或者连续多次，连续次数上不封顶。 {n} 表示前面的字符连续出现n次，将会被匹配到。 {x,y} 表示之前的字符至少连续出现x次，最多连续出现y次，都能被匹配到，换句话说，只要之前的字符连续出现的次数在x与y之间，即可被匹配到。 {,n} 表示之前的字符连续出现至多n次，最少0次，都会被匹配到。 {n,}表示之前的字符连续出现至少n次，才会被匹配到。 分组与后向引用( ) 表示分组，我们可以将其中的内容当做一个整体，分组可以嵌套。(ab) 表示将ab当做一个整体去处理。\1 表示引用整个表达式中第1个分组中的正则匹配到的结果。\2 表示引用整个表达式中第2个分组中的正则匹配到的结果。 3.awk awk的基本语法结构 awk [options] ‘Pattern {Action}’ file1,file2 如果是对命令执行awk操作而不是文件，那么可以使用命令+管道符（|）+awk 3.1基础入门df |awk ‘{print $5}’ 表示输出df信息的第五列 df |awk ‘{print $4,$5}’ 表示输出df信息的第四和第五列 除了输出文本或命令输出信息中的列，我们也可以自己添加信息，只需在””中添加即可 awk ‘{print “anychar:”$4,”anychar2”$5}’ file1 如果写成如下格式，那么输出的就不是第五列，而是输出$5 df |awk ‘{print “$5”}’ Pattern中文含义是模式，特殊的2种模式BEGIN和END 在执行文件1（file1）前，先执行BEGIN的内容，即先把aaa输出，然后再对文件1做处理，处理方式就是后面写的动作 awk ‘BEGIN{print “aaa”} {print “anychar:”$4,”anychar2”$5}’ file1 在执行完对文件1（file1）的操作后，再在结尾执行END的内容，即输出aaa awk ‘{print “anychar:”$4,”anychar2”$5} END{print “aaa”}’ file1 3.2分隔符3.2.1输入分割符文本输入的时候，默认是以空格作为分割符号的，但是我们可以手工指定在文本中的一行，该以什么符号作为区分一列的分割符 awk -F# ‘{print $4,$5}’ file1 通过-F参数，来指定#作为输入分割符 awk -v FS=’#’ ‘{print $4,$5}’ file1 通过-v参数，来修改系统内置的输入分割符变量，效果同上 3.2.2输出分割符文本输出的时候，默认也是以空格作为分割符号的，但是我们可以指定其他的符号来连接不同的列 awk -v OFS=’++++’ ‘{print $4,$5}’ file1 通过-v参数，来修改系统内置的输出分割符变量 注：输出分割时，用“，”来区分。如果是想把两列连在一起，可以写成以下形式 awk ‘{print $4$5}’ file1 3.3awk变量3.3.1内置变量 名称 代码 输入分割符 FS 输出分隔符 OFS 输入记录分隔符 RS 输出记录分割符 ORS 当前行被分割成了几列 NF 行号 NR 各文件分别记录行号 FNR 当前文件名 FILENAME 命令行参数的个数 ARGC 保存的是命令行所给的各参数 ARGV awk ‘{print NR,NF}’ file1 表示输出文件1中的行号及该行的列数 FS和OFS都是以每行各个字符间隔为单位 RS和ORS是来判断以什么条件来作为换行，默认情况下，系统认为是回车即换行 3.3.2自定义变量awk -v 变量名称=”变量值” eg： awk -v mychar=”hahaha” ‘BEGIN｛print mychar｝’ or： awk -v ‘BEGIN｛mychar=”hahaha” ; print mychar｝’ 3.4格式化 3.5模式 使用正则表达式 awk ‘/正则表达式/ {print $0}’ file1 3.6动作 4.sed awk的基本语法结构 awk [options] ‘动作’ 4.1新增与删除sed ‘sed 2,5d’ 其中d表示删除，意味删除2到第5行 1234567891011121314151617181920212223242526[root@vultr ~]# nl /etc/passwd 1 root:x:0:0:root:/root:/bin/bash 2 bin:x:1:1:bin:/bin:/sbin/nologin 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin 4 adm:x:3:4:adm:/var/adm:/sbin/nologin 5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 6 sync:x:5:0:sync:/sbin:/bin/sync 7 shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown 8 halt:x:7:0:halt:/sbin:/sbin/halt 9 mail:x:8:12:mail:/var/spool/mail:/sbin/nologin 10 operator:x:11:0:operator:/root:/sbin/nologin 11 games:x:12:100:games:/usr/games:/sbin/nologin 12 ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin 13 nobody:x:99:99:Nobody:/:/sbin/nologin 14 systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin 15 dbus:x:81:81:System message bus:/:/sbin/nologin 16 polkitd:x:999:998:User for polkitd:/:/sbin/nologin 17 ntp:x:38:38::/etc/ntp:/sbin/nologin 18 sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin 19 postfix:x:89:89::/var/spool/postfix:/sbin/nologin 20 chrony:x:998:996::/var/lib/chrony:/sbin/nologin 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# [root@vultr ~]# nl /etc/passwd | sed '2,20d' 1 root:x:0:0:root:/root:/bin/bash 21 tcpdump:x:72:72::/:/sbin/nologin sed ‘2a 字符串’，在第2行后面插入一字符串，i表示在第n行前插入字符串 12345678910111213141516171819202122[root@vultr ~]# nl /etc/passwd | sed '2,20d' |sed '1a nihaoa' 1 root:x:0:0:root:/root:/bin/bashnihaoa 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# [root@vultr ~]# [root@vultr ~]# nl /etc/passwd | sed '2,20d' |sed '1i nihaoa'nihaoa 1 root:x:0:0:root:/root:/bin/bash 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# [root@vultr ~]# [root@vultr ~]# nl /etc/passwd | sed '2,20d' |sed '2i nihaoa' 1 root:x:0:0:root:/root:/bin/bashnihaoa 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# [root@vultr ~]# [root@vultr ~]# nl /etc/passwd | sed '2,20d' |sed '20i nihaoa' 1 root:x:0:0:root:/root:/bin/bash 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# 想要增加多行，可以先输入\，然后回车，就会跳转到下一行。 1234567891011121314[root@vultr ~]# nl /etc/passwd | sed '2,19d' |sed '2a nihaoa \&gt;zaijian' 1 root:x:0:0:root:/root:/bin/bash 20 chrony:x:998:996::/var/lib/chrony:/sbin/nologinnihaoa &gt;zaijian 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# [root@vultr ~]# nl /etc/passwd | sed '2,19d' |sed '2a nihaoa \&gt; zaijian' 1 root:x:0:0:root:/root:/bin/bash 20 chrony:x:998:996::/var/lib/chrony:/sbin/nologinnihaoa zaijian 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# 4.2替代sed ‘2,19c 字符串’，c表示从2到19行被该字符串替代掉 123456[root@vultr ~]# nl /etc/passwd | sed '2,19c 2 to 19 no show' 1 root:x:0:0:root:/root:/bin/bash2 to 19 no show 20 chrony:x:998:996::/var/lib/chrony:/sbin/nologin 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# sed -n ‘2,4p’，表示显示第2到第4行，-n参数需要加上 123456789101112131415161718192021222324252627282930[root@vultr ~]# nl /etc/passwd | sed '2,4p' 1 root:x:0:0:root:/root:/bin/bash 2 bin:x:1:1:bin:/bin:/sbin/nologin 2 bin:x:1:1:bin:/bin:/sbin/nologin 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin 4 adm:x:3:4:adm:/var/adm:/sbin/nologin 4 adm:x:3:4:adm:/var/adm:/sbin/nologin 5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 6 sync:x:5:0:sync:/sbin:/bin/sync 7 shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown 8 halt:x:7:0:halt:/sbin:/sbin/halt 9 mail:x:8:12:mail:/var/spool/mail:/sbin/nologin 10 operator:x:11:0:operator:/root:/sbin/nologin 11 games:x:12:100:games:/usr/games:/sbin/nologin 12 ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin 13 nobody:x:99:99:Nobody:/:/sbin/nologin 14 systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin 15 dbus:x:81:81:System message bus:/:/sbin/nologin 16 polkitd:x:999:998:User for polkitd:/:/sbin/nologin 17 ntp:x:38:38::/etc/ntp:/sbin/nologin 18 sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin 19 postfix:x:89:89::/var/spool/postfix:/sbin/nologin 20 chrony:x:998:996::/var/lib/chrony:/sbin/nologin 21 tcpdump:x:72:72::/:/sbin/nologin[root@vultr ~]# [root@vultr ~]# nl /etc/passwd | sed -n '2,4p' 2 bin:x:1:1:bin:/bin:/sbin/nologin 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin 4 adm:x:3:4:adm:/var/adm:/sbin/nologin sed ‘s/需要被替代的字符串/替换后的字符串/g’ 4.3直接在文本中做修改操作]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[iptables笔记]]></title>
    <url>%2F2019%2F08%2F11%2Fiptables%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[iptables笔记 朱双印博客-iptables规则 数据经过防火墙流程图 1.查找规则 iptables -t 表名 -L iptables -t 表名 -L 链名 #-L显示链名下的表名规则 iptables -t 表名 -vL 链名 #-v显示详细信息 iptables -t 表名 -nvL 链名 #-n不进行地址解析 iptables -t 表名 -xnvL 链名 #-x显示精确计数值 iptables –line -t 表名 -xnvL 链名 #–line在规则前加上序列号 2.添加规则 iptables -t 表名 -A 链名 匹配条件 -j 动作 iptables -t filter -A INPUT -s 192.168.1.1 -j DROP #-A插入到规则最后 iptables -t 表名 -I 链名 匹配条件 -j 动作 iptables -t filter -I INPUT -s 192.168.1.1 -j ACCEPT #-I插入到规则开头 iptables -t 表名 -I 链名 规则序号 匹配条件 -j 动作 iptables -t filter -I INPUT 5 -s 192.168.1.1 -j REJECT #在第五行规则中插入 iptables -t 表名 -P 链名 动作 iptables -t filter -P INPUT REJECT #设置INPUT链中filter表的默认规则 3.删除规则 按规则序号删除iptables -t 表名 -D 链名 规则序号 iptables -t filter -D INPUT 5 按匹配条件与动作删除 iptables -t 表名 -D 链名 匹配条件 -j 匹配动作 iptables -t filter -D INPUT -s 192.168.1.1 -j ACCEPT 删除某个链下指定表的所有规则 iptables -t 表名 -F 链名 iptables -t filter -F INPUT 删除所有链下指定表的所有规则 iptables -t 表名 -F iptables -t filter -F 4.修改规则如果要修改规则，必须要指明原规则中的匹配条件（或者理解为只能修改动作） iptables -t 表名 -R 链名 规则序号 规则原始匹配条件 -j 动作 iptables -t filter -R INPUT 3 -S 192.168.1.1 -J ACCEPT 另外一种方式是，先删除规则。再在原来的位置添加规则 5.保存规则 6.匹配条件6.1基本匹配条件 -s用于匹配源IP地址。可以指定多个IP地址，多个IP地址间用“，”号隔开；也可以指定IP网段 iptables -t filter -I INPUT -s 192.168.1.111,192.168.1.118 -j DROP iptables -t filter -I INPUT -s 192.168.1.0/24 -j ACCEPT iptables -t filter -I INPUT ! -s 192.168.1.0/24 -j ACCEPT -d用于匹配目的IP地址。可以指定多个IP地址，多个IP地址间用“，”号隔开；也可以指定IP网段 iptables -t filter -I OUTPUT -d 192.168.1.111,192.168.1.118 -j DROP iptables -t filter -I INPUT -d 192.168.1.0/24 -j ACCEPT iptables -t filter -I INPUT ! -d 192.168.1.0/24 -j ACCEPT -p用于匹配协议类型，常见的匹配类型有TCP、UDP、ICMP、ESP、AH等 iptables -t filter -I INPUT -p tcp -s 192.168.1.146 -j ACCEPT iptables -t filter -I INPUT ! -p udp -s 192.168.1.146 -j ACCEPT -i表示从哪个网卡接口流入本机，不能用于output链和postrouting链 -o表示从哪个网卡接口流出本机，不能用于prerouting链和input链 6.2扩展匹配条件(如果协议和扩展模块一致，扩展模块可省略)TCP扩展模块： -p tcp -m tcp –sport，用于匹配协议源端口，可以用冒号”:”指定一个连续的端口范围(udp类似) -p tcp -m tcp –dport，用于匹配协议目的端口，可以用冒号”:”指定一个连续的端口范围（udp类似） iptables -t filter -I OUTPUT -d 192.168.1.146 -p tcp -m tcp --sport 22 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m tcp --dport 22:25 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m tcp --dport :22 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m tcp --dport 80: -j REJECT iptables -t filter -I OUTPUT -d 192.168.1.146 -p tcp -m tcp ! --sport 22 -j ACCEPT –tcp-flags用于匹配tcp头部中的标志位 iptables -t filter -I INPUT -p tcp -m tcp --dport 22 --tcp-flags SYN,ACK,FIN,RST,URG,PSH SYN -j REJECT iptables -t filter -I OUTPUT -p tcp -m tcp --sport 22 --tcp-flags SYN,ACK,FIN,RST,URG,PSH SYN,ACK -j REJECT iptables -t filter -I INPUT -p tcp -m tcp --dport 22 --tcp-flags ALL SYN -j REJECT iptables -t filter -I OUTPUT -p tcp -m tcp --sport 22 --tcp-flags ALL SYN,ACK -j REJECT –syn,相当于使用了“–tcp-flags SYN,ACK,FIN,RST SYN” iptables -t filter -I INPUT -p tcp -m tcp --dport 22 --syn -j REJECT multiport扩展模块： -p tcp -m multiport –sports，用于匹配协议源端口，可以用逗号”,”指定多个离散端口 -p tcp -m multiport –dports，用于匹配协议目的端口，可以用逗号”,”指定多个离散端口 iptables -t filter -I OUTPUT -d 192.168.1.146 -p udp -m multiport --sports 137,138 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport --dports 22,80 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport ! --dports 22,80 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport --dports 80:88 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport --dports 22,80:88 -j REJECT icmp扩展模块（略） state扩展模块（略，但是概念重要） 7.匹配动作 动作SNAT，进行源地址转换(公网是固定IP) iptables -t nat -A POSTROUTING -s 10.1.0.0/16 -j SNAT --to-source 公网IP 动作MASQUERADE，进行源地址转换（公网是动态IP） iptables -t nat -A POSTROUTING -s 10.1.0.0/16 -o eth0 -j MASQUERADE 动作DNAT，进行目的地址转换 iptables -t nat -I PREROUTING -d 公网IP -p tcp --dport 公网端口 -j DNAT --to-destination 私网IP:端口号 iptables -t nat -I PREROUTING -d 公网IP -p tcp --dport 8080 -j DNAT --to-destination 10.1.0.1:80 iptables -t nat -A POSTROUTING -s 10.1.0.0/16 -j SNAT --to-source 公网IP 动作REDIRECT，进行本机端口重定向 iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[新东方赵丽托福英语词汇8000-01]]></title>
    <url>%2F2019%2F07%2F18%2F%E6%96%B0%E4%B8%9C%E6%96%B9%E8%B5%B5%E4%B8%BD%E6%89%98%E7%A6%8F%E8%8B%B1%E8%AF%AD%E8%AF%8D%E6%B1%878000-01%2F</url>
    <content type="text"><![CDATA[新东方赵丽托福英语词汇8000-01共30词 compel repel insidious assist timid toxic compulsive repulsive preside resist intimidate intoxicate compulsion repulsion resident insist Fawn Pawn expel impel Vigor persist Spawn Brawn propel dispel invigorate Lawn Yawn Dawn 4.词根词缀法词根:(本义) body 前缀:改变含义不变词性 anti(反)+body(体)-抗体 后缀:改变词性,不变含义 pass passable 词性转换库: pel表示推（verb）—pulsive（形容词）—pulsion（名词） compel （com共同+pel） verb:强迫，迫使 The law can compel fathers to make regular payments for their children.这项法律可强制父亲定期支付子女的费用。 compulsive( adjective ):难以制止的；难控制的 compulsive eating/spending/gambling强迫性进食╱消费；上瘾的赌博 The programme made compulsive viewing.这节目引人入胜，收看起来欲罢不能。 compulsion（noun）:强迫；强制 compulsion(on sb) to do sth There are no compulsions on students to attend classes.没有强求学生上课。 repel(re向后+pel) verb:击退；驱逐 Troops repelled an attempt to infiltrate the south of the island.部队挫败了对该岛南部的潜入企图。 expel (ex老大+pel) verb:把…开除（或除名）;驱逐出境;排出；喷出 1.She was expelled from school at 15.她 15 岁时被学校开除了。 2.Foreign journalists are being expelled.外国记者被驱逐出境。 impel (im内心深处+pel) verb:迫使(激励) He felt impelled to investigate further.他觉得有必要作进一步调查。 propel (pro向前+pel) verb:推动；驱动；推进 He succeeded in propelling the ball across the line.他成功地把球带过线。 Fury propelled her into action.怒火驱使她行动起来。 dispel(dis分开+pel) verb:驱散，消除（尤指感觉或信仰） His speech dispelled any fears about his health.他的发言消除了人们对他身体健康的担心。]]></content>
      <categories>
        <category>英语</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[新东方赵丽托福英语词汇8000-00]]></title>
    <url>%2F2019%2F07%2F15%2F%E6%96%B0%E4%B8%9C%E6%96%B9%E8%B5%B5%E4%B8%BD%E6%89%98%E7%A6%8F%E8%8B%B1%E8%AF%AD%E8%AF%8D%E6%B1%878000-00%2F</url>
    <content type="text"><![CDATA[新东方赵丽托福英语词汇8000-00共18词 haunt flaunt daunt vaunt gaunt jaunt journey saunter taunt ponderous slurp drone quaff ambition schedule pajama famine sanguine 1.拆分法—aunt系列haunt(home+aunt) verb:(鬼魂)出没;(不快的事情)萦绕 1.A headless rider haunts the country lanes.一个无头骑士常出没于乡间的小路上。 2.The memory of that day still haunts me.我的脑海中常常回想起那天的情景。 noun:常来常往的地方 The pub is a favourite haunt of artists.这家酒吧是艺术家最爱光顾的地方。 flaunt(fly+aunt) verb:炫耀，夸耀 He did not believe in flaunting his wealth.他不相信摆阔有什么好处。 daunt(da打+aunt) verb:恐吓；使胆怯；使气馁；使失去信心 She was a brave woman but she felt daunted by the task ahead.她是一个勇敢的女人，但对面前的任务却感到信心不足。 vaunt(v5+aunt) 不及物动词夸耀, 吹嘘[…] [of, over, about]~ of one’s skill夸耀自己的技巧 可数名词自夸, 夸张, 吹嘘make a ~ of﹍ 夸耀… gaunt(gre+aunt) adjective:瘦削憔悴的（常因疾病、饥饿或忧虑） a gaunt face憔悴的面容 jaunt(接+aunt) noun:短途旅行 a weekend jaunt 周末小旅行 ps:journey接你：长途旅行 saunter(see+aunter)姑姑到处乱看 verb:闲逛 He sauntered by, looking as if he had all the time in the world.他悠闲地走过，仿佛时间对他来说是无穷无尽的。 taunt(吐，呸+aunt) verb:辱骂；嘲笑 The other kids continually taunted him about his size.其他孩子不断地耻笑他的个头儿。 noun:嘲笑（或讽刺、奚落等）的言辞 Black players often had to endure racist taunts.黑人运动员经常得忍受种族歧视性的奚落。 2.谐音联想法ponderous(胖的要死) adjective:笨重的，缓慢的 She watched the cow’s ponderous progress.她看着牛迟缓地向前走着。 slurp(嗖的一下) verb:（喝东西时）发出啧啧的声音 He was slurping his tea.他正咂着嘴喝茶。 drone(juan～juan～,蜜蜂等嗡嗡的声音) noun:嗡嗡声 the distant drone of traffic远处车辆往来发出的嗡嗡声 quaff(夸父追日饮一河之水；马和骡子kuafukuafu喝水的声音) verb:豪饮；痛饮；开怀畅饮 ambition(俺必胜) noun:追求的目标;野心 1.It had been her lifelong ambition.这是她终身追求的目标。 2.She was intelligent but suffered from a lack of ambition.她很聪明，但却缺乏远大志向。 3.拼音联想法schedule(s+che+du+le,该死的车又堵了，我们不得不改变我们的计划表) noun:工作计划；日程安排 I have a hectic schedule for the next few days.我今后几天的日程紧得要命。 pajama(pa+ja+ma,趴在家妈妈穿的) noun:睡衣 famine(fa+mi+ne,发米呢) noun:饥荒 a severe famine严重饥荒 sanguine(san+gui+ne,三桂呢) adjective:充满信心的；乐观的;面色红润的 hey are less sanguine about the company’s long-term prospects.他们对公司的远景不那么乐观。]]></content>
      <categories>
        <category>英语</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[远程桌面访问不了]]></title>
    <url>%2F2019%2F07%2F07%2F%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2%E8%AE%BF%E9%97%AE%E4%B8%8D%E4%BA%86%2F</url>
    <content type="text"><![CDATA[远程桌面访问不了问题：总部PC去访问分支的服务器3389桌面，会出现不定时中断的问题 10.0.2.132为PC 10.16.2.200为远程桌面的服务器 拓扑环境 总部有三层环境，AC透明模式部署，分支二层环境，就一个网段 数据包是总部访问分支访问不了的时候抓取的数据包，要求通过数据包，分析问题原因。 排查思路： 只抓取了分支vpn上eth0口的包和总部vpn设备eth0上的包（建议是分支电脑和总部服务器都抓取，抓不了也没 办法） 因为是终端pc访问服务器的时候经常断开。所以先看在终端侧的数据包，也即是查看zbssleth0这个包 打开之后，先点击Statistics，选中conversation 出现下图界面，选中TCP，看到存在三个连接。说明应该是测试过3次。其中注意到字节数Bytes， 第二个连接有112kB，比其他连接传输的数据都多，那我们就先来看这个连接。 怎么看呢？右键该连接后，照下图操作 wireshark自动过滤该连接，显示如下界面 我们可以点开专家分析来看看，是否存在什么问题。点开之后，记得把Limit to Display ﬁlter 给勾上，这样才会显示在wireshark过滤后展示页面的信息。 由于不涉及访问卡慢的问题，所以此时我们没必要看重传等信息。重点应该关注是什么导致的连接 断开。此时，明显能看到告警信息中，存在RST包，那我们就可以看下304号包与306号包。 找到304号包与306包后，发现后面就再也没有数据了，说明此时连接是中断了。明明数据传的好好 的，怎么突然客户端10.0.2.132就发送一个RST包呢。此时需要注意的是，我们的数据包是在总部vpn设备上eth0口抓的，也就是说这个流量是从内部传过来的。结合着总部的拓扑情况，出现中断， 可能存在2种情况。要么是客户端自己的问题，要么就是有啥子设备替客户端发送了这个RST包。 一个直接的办法就是在客户端上抓包。看下客户端有没有发出这个RST包。如果没有，结合着网络拓 扑，那基本就可以确定是AC发了这个RST包了（第一个，客户环境比较简单；第二个，谁叫AC是行 为管控呢）。如果客户端上的抓包有这个RST，那就是客户端的问题。 如果客户说，我电脑上不能安装其他软件（不管为啥，你就当死活不允许好了）。那就是说，我们 不能在客户端上抓包。那好，我们就只能在现有的数据包中获取蛛丝马迹了。 此时，我们还有一个方法判断是不是由客户端发出的这个RST包，那就是通过IP.ID这个参数。 IP.ID是什么鬼？那我们要知道数据包是如何封装的，如下图所示。如果看不懂，请参考网络基本功。IP.ID是IP数据报头中的一个字段。表明了一个数据包的身份，比如一个数据内容过大，被传输的时候就要对这个数据进行切割，即进行分片。当接收方收到这些分片后，需要把这些分片组装起来，那接收方每次收这么多的数据包，它咋知道哪些分片是从同一个数据内容中出来的呢？就要靠IP.ID了，通过IP.ID就可以把这些分片组装起来，还原成最初始的大块数据。就像你玩乐高玩具，乐 高里面这么多零件，如果这些零件中混入了其他玩具的零件。那你想把这个乐高玩具组装起来肯定需要一个标识来进行识别哪些零件是同属于一个乐高模型的。 上述还只是告诉了你，啥是IP.ID。那回到问题中来，你还需要知道的一个点就是，如果是一个设备 去发数据包，那么他的IP.ID增长是线性的。即一般来说，同一台设备发出第一个包，如果IP.ID是1， 第二个包就是2。就算不符合这规律，那数值起码也不会差太多。 我们再来看下wireshark，299号包的IP.ID=18970。302号包的IP.ID=18971。而304号包的IP.ID=22566。那RST包和之前的两个包，这数值也差的太多了啊，所以基本能判断这个包不是客户端发出的。]]></content>
      <categories>
        <category>实战</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[视频会议卡顿]]></title>
    <url>%2F2019%2F06%2F13%2F%E8%A7%86%E9%A2%91%E4%BC%9A%E8%AE%AE%E5%8D%A1%E9%A1%BF%2F</url>
    <content type="text"><![CDATA[视频会议卡顿——用wireshark定位问题基础知识（TCP和UDP的比较） TCP报文和UDP报文如下 在报文头部中，我们可以知道，TCP除了源目端口，还会有seq号，ack号，6个标志位以及一个接受窗口字段。而UDP报文头部只有源目端口，长度和校验和字段。UDP报文头部8字节，TCP的最短报文头部20字节，UDP报文头部连TCP头部长度的一半都不到，想复杂也复杂不起来。 我们知道TCP是需要建立连接后才会开始传输数据的，而UDP不用。TCP就像打电话，必须先拨通对方手机，然后才互相交流，如果对方没有听清，那自己就可以把话语重复一遍，确保对方接收无误。UDP就像发短信，我给一个人发送短信后，第一不会知道对方是否有收到；即便对方收到后，我也不知道信息发给对方有没有出错。 在TCP中，假设有很长一段数据要发送，假设有4380（1460*3）字节的数据。我们知道在以太网中，以太网帧的数据部分最大长度是1500字节，假设TCP和IP头部各占20字节，那么一个TCP段的数据就是1460字节，超过这个字节就要分段。所以这4380个字节就要分成3个包去发送。这3个包就类似于下表，假设第一个包的seq号是0，那么下一个包的seq号就是1460（上一个包的seq号+长度）。 包号 seq号 lenth长度 1 0 1460 2 1460 1460 3 2920 1460 正常情况下，接收方要收到这3个包。假设第二个包在传输过程中丢失了，接收方只能收到seq号是0（长度是1460字节）和seq号是2920（1460字节）的包，那他就清楚1460的这个包在路上丢失了，就可以要发送方重传第二个包。 对于UDP而言，它没有建立连接的机制，同时也没有流控和差错控制机制。那它要发送数据出去，如果数据部分超过了最大的数据长度1472字节（以太网帧的数据部分最大长度是1500字节，IP头部20字节，UDP头部8字节），就要靠下层的IP来分片。分片要如何组装起来，在之前的文章中提到过，涉及的就是ip.id和片偏移。 如果数据部分没有超过UDP中的最大数据长度，就不会被分片，那么每个报文的ip.id也就是不一样的。 也就是说在UDP中，发送方发送一个小数据出去，接收方收到就收到了，没收到那我也不知道，也不会重传。 而发送方要是发送一个大数据（超过UDP最大数据长度，会有多个分片），如果有一个分片丢失，那么接收方按ip.id和片偏移无法组装起来，那么就会向发送方发送消息，让发送方重传。此时的重传不会像TCP一样，只发送丢弃的那个包，而是要把之前这个包的所有分片全部重传。 客户问题 左边是分支，右边是总部。分支的视频服务器上看总部端的画面很流程，但是在总部的视频服务器上看分支端的画面则特别卡。 客户拓扑 问题分析 视频会议和语音通话基本都是使用UDP协议。同时数据字节不会很大，一般不会超过最大UDP数据报文长度，那么每个数据包ip.id的值是不一样的。不会出现设备收到分片组装不起来的情况。 分支的视频服务器看总部的画面正常，说明总部给分支传的UDP数据流是没有丢包的。 总部的视频服务器看分支画面有卡顿，说明分支给总部传的UDP数据流可能存在丢包。 总部和分支之间互传数据是互不干扰的。因为不是TCP下，建立连接后的两端数据互传。 两台设备上都做了策略路由，视频服务器的流量都走了联通线路。（其实最开始的情况是，分支到总部的数据往电信线路传了，总部往分支的数据就走了联通，存在总部看分支画面丢包的情况。怀疑是线路问题，就调整了策略，让数据都走了联通，但是问题还是存在。） 查看两端的控制台配置，策略都是正常的。出现丢包的时候，经过设备的流量都不大，cpu利用率也不高。 于是在客户两端都开启视频服务器的情况下，抓包分析。 先看分支的内网口（eth0）的抓包情况。 由于是总部看分支，画面存在卡顿。那我们主要关注的就是分支视频服务器172.17.160.8给总部视频服务器10.16.121.250这个方向的流量 选择B到A这个方向。选择完后，主界面就会自动给你过滤出分支视频服务器给总部视频服务器这个方向的流量 由于画面存在卡顿，很可能的原因是丢包。UDP不像TCP那样，有所谓的seq号。在TCP中，哪个包丢了，我可以通过seq号把丢的包找到，但UDP不行。那有没有什么办法可以让UDP像TCP一样，能给这些数据包按顺序编个号吗？哪个UDP包丢了，我可以通过这个编号识别到。 是可以的。那就是把UDP的数据包编码为RTP的数据包，对RTP协议感兴趣的同学可以看这篇文章实时传输协议RTP/RTCP 我们来看下把UDP包编码成RTP包在wireshark中是啥样的。 上图看到，其实所谓的编码，是把UDP包中的数据部分变成了RTP数据报文，RTP数据报文中存在seq号。 我们在会话统计中可以看到分支给总部传数据的时候，建立了6个连接。其中前2个连接的数据量相对大一些。 为什么前2个连接，传输的数据多些呢？ 我们先选择第一个连接，即端口49152到端口2326的。把这个连接使用的UDP协议编码为RTP协议 编码之后，此时wireshark界面如图所示 接下来，我们打开RTP流分析。可以看到面板已经给我们统计出有2个丢包了，你还能分析出是丢了哪两个包。 16.由于是打开的分支eth0接口的抓包，所以此时就可以说明，丢包是丢在了内网 按照同样的方法，我们还要查看下在公网链路上是否存在丢包，以及在总部内网是否存在丢包。 因为分支内网存在丢包，所以在查看公网链路上的数据包时（即fzeth3和zbeth2），如果公网数据包中，丢包的序列号和在fzeth0数据包内的一致，说明公网是没有丢包的。如果除去内网中丢包的序列号，还有其他序列号丢失，说明公网链路也有问题。 从fzeth3的数据包中，可以看到丢了2个，也是seq=34591和seq=34637丢包了。和内网抓包fzeth0丢包情况一致。而在总部设备的2个口抓包情况也是一样的，就不赘述了。查看方法和上述一致。 定位了问题后，可以判断分支内网存在丢包。那么有条件的话就可以在分支视频服务器及交换机上抓包对比查看。 最后定位出的问题是交换机网口有问题，换一个交换机问题就解决了。]]></content>
      <categories>
        <category>实战</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[并行/串行/异步传输]]></title>
    <url>%2F2019%2F06%2F12%2F%E5%B9%B6%E8%A1%8C-%E4%B8%B2%E8%A1%8C-%E5%BC%82%E6%AD%A5%E4%BC%A0%E8%BE%93%2F</url>
    <content type="text"><![CDATA[【转载】什么是并行传输、串行传输、异步传输？ 文章收录自互联网，著作权归原作者所有，若有冒犯请联系我删除。 商业转载请联系作者获得授权，非商业转载请注明出处。 本人转载只作个人学习及后续查阅使用 该篇文档作者：航空航天迷 原文链接：什么是并行传输、串行传输、异步传输？ 来源：知乎 传输可以分为串行传输和并行传输。 串行传输可分为异步传输、同步传输和等时传输。 下图2是并行传输的示意图： 在并行传输中，使用多根并行的数据线一次同时传输多个比特。 例如在图2中，共有8根数据线，一次同时传输8个比特，每个比特占用一根数据线。 下图3是串行传输的示意图： 在串行传输中，使用一根数据线传输数据，一次传输1个比特，多个比特需要一个接一个依次传输。 下面简单介绍一下串行传输与并行传输的发展历史，从而了解他们俩各自的优缺点，以及发展趋势。 最早的计算机（电子管计算机和晶体管计算机），其各个接口，例如输入接口、输出接口和存储器接口等，一般采用串行接口，以串行传输的方式传输数据。 下图4为电子管计算机ENIAC 电子管计算机ENIAC诞生于美国宾夕法尼亚大学。重30多吨，占地约170平方米，装有约18000只电子管。 下图5为电子管计算机中使用的电子管 图5中小玻璃瓶状的东西就是电子管，电子管的体积比较大 下图6为晶体管计算机TRADIC 1954年，贝尔实验室使用800只晶体管组装了世界上第一台晶体管计算机TRADIC。相比电子管，晶体管体积小、重量轻、寿命长、发热少、功耗低，大大改进了计算机中的电子线路的结构，大幅度提高了运算速度。 下图7为晶体管计算机TRADIC中使用的晶体管 电子管计算机和晶体管计算机以串行传输的方式传输数据，其原因是当时各个部件都是分立的部件，而不是像今天这样使用集成电路设计。如果采用并行线路的话，元件的数量和占用的空间将成倍增长。比如，一个8比特并行线路的元件数量是串行线路的元件数量的8倍（因为需要为每根线路配置一套接收元件）。另外，元件的数量成倍增长的话，耗电量也会大幅增加。 集成电路技术出现后，大量元件可以集成到一个小小的芯片上，并行传输变得方便而便宜。不论是8比特、16比特还是更高比特位数的并行线路，只需要一个并行接口芯片就可以处理，而且并行接口芯片只比串行接口芯片贵一点。 除了方便便宜外，在相同的工作频率下并行传输的传输速度是串行传输的数倍，迎合了人们对速度的追求，所以硬盘、打印机等设备开始使用并行传输以提高传输速度。PATA（Parallel Advanced Technology Attachment，并行高级技术附件）接口、并口（Parallel Port）和PCI（Peripheral Component Interconnect，外设部件互连）接口成为流行的并行接口。 下图8为电脑主板上的PATA接口（并行传输），用于连接硬盘和光驱 下图9为用于PATA接口的连接线缆 下图10中的编号2为电脑上的并口（并行传输），用于连接打印机、扫描仪等 下图11为电脑主板上的PCI接口（并行传输），用于插接外置网卡、声卡、显卡和调制解调器卡等） 下图12为采用PCI接口的显卡 图12中的显卡使用时是插入图11中的PCI接口（插槽）中。 在相同的工作频率下并行传输的传输速度是串行传输的数倍，但并行线路有一些难以克服的缺点，导致依靠并行线路的并行传输无法用于长距离通信。计算机与外界的长距离通信，例如与网络中的另外一台计算机进行通信时，只能使用串行传输。 在计算机内部，如今串行传输也显示出了它的优势，有取代并行传输的趋势。 例如， SATA接口取代PATA接口； USB接口取代并口； PCI Express接口取代PCI接口。 为什么串行传输有取代并行传输的趋势呢？ 与串行传输相比，并行传输的缺点是： 一、线路的成本高 并行传输如果每个时钟节拍发送16个比特，则需要16根数据线（另外还需要多根控制线）。 PATA（并行传输）连接线缆包含40根导线（16根数据线，24根用于接地和进行控制）； SATA（串行传输）连接线缆包含7根导线（4数据线+3接地线）。 如果是长距离通信，并行传输的线路成本是串行传输的若干倍。 另外，只有一对传输线时，串行传输也可以实现双向通信，所以可以直接利用现有的电话线路进行数据传输；而并行传输要多根并行的传输线，没有现有的线路可以利用，要另外专门铺设线路，成本高。 二、体积大 并行接口占用空间大，对应线缆占用空间也大。 如图14所示，PATA的接口与连接线缆的尺寸大大于SATA的接口与连接线缆的尺寸 如果是长距离通信，要求使用比较粗的信号线，以便降低信号的衰减，并行传输需要使用多根较粗的信号线捆扎在一起组成通信线缆，占很大空间。 即使是计算机内部通信，并行传输的线缆所占用的空间也比串行传输的线缆所占用的空间大很多。 并行接口的尺寸比串行接口的尺寸大很多，则不利于设备的小型化。例如，在手机和穿戴式设备等领域，希望零件的尺寸越小越好。 三、信号线之间的干扰大，不能用于长距离传输 并排的信号线在进行高速传输时，会在每条信号线的周围产生微弱的电磁场，出现串音干扰，进而影响到其它信号线中的数据传输。传输距离越长，串音干扰越严重。 所以，PATA线缆的长度不能超过0.4米，而SATA线缆可以达到1米。 四、并行传输具有同步问题 并行传输中，如果并行的线路之间的物理性质不一致，例如长度上有细微差别，会导致并行线路中传输的比特不是同时到达接收方，接收器接收数据时容易出错。 五、传输频率低 在并行传输中，如果传输频率高的话，数据线之间会产生很大的干扰，造成数据出错，即使为数据线添加屏蔽层，也不能保证屏蔽掉高频率产生的干扰。所以，并行传输的最高传输频率有一定限制。 PATA接口的最高传输频率为33MHz，这个几乎已经达到了并行接口的极限。 串行传输每次只传输一个比特，但是它的传输频率可以非常高，达到10GHz，是33MHz的300倍。相当于并行传输每发送1次，串行传输可以发送300次。并行传输每次发送300比特，才能赶上串行传输的速度，但是每次发送300比特，就需要300根并行的数据线，这是不现实的。 因为并行传输和串行传输各自的这些优缺点，导致并行传输仅仅用于短距离传输，而长距离传输则采用串行传输；同时，在短距离传输中，串行传输也在逐步取代并行传输。 什么是异步传输呢？ 计算机的键盘与主机之间的数据传输就是异步传输。 在键盘上按下一个字母键、数字键或特殊字符键，键盘就需要向主机发送一个对应的长度为8比特的ASCII字符，这个8比特的ASCII字符就是需要发送的数据，大小为1个字节。 如上图所示：例如，当用户按下小写字母键“k”，键盘需要向主机发送字符“01101011” 用户可能在任意时刻按键盘，所以键盘向主机发送数据的时间不是固定的，也不会事先约好，任何时刻都有可能发送。 主机事先并不知道键盘什么时候会给自己发送数据，只能静静地等待，一旦发现键盘向自己发送数据，则马上接收。 主机如何发现键盘开始向自己发送数据了呢？ 当键盘不需要向主机发送有效数据时，也就是键盘处于空闲（idle）时，键盘会连续不断地向主机发送比特“1”，告诉主机自己处于空闲状态。比特“1”用正电平表示，也就是键盘一直向主机发送正电平，表示当前没有有效数据发送给主机。 当键盘被按下，键盘需要向主机发送数据时，键盘会先在数据前添加比特“0”，组成新字符，再发送。 例如，如上图所示，发送数据“01101011”之前，先在“01101011”的前面添加“0”，组成新字符“011010110” 比特“0”可以用零电平表示。 如上图所示，键盘处于空闲时，主机接收的一直是比特“1”（正电平），当主机突然接收到比特“0”（零电平）时，马上反应过来，键盘在向自己发送有效数据，则主机开始接收“0”后面的有效数据，这个“0”相当于这个新字符的起始位（start bit）。 主机开始接收有效数据后，怎么才能知道有效数据接收完了要停下来呢？ 为了解决这个问题，键盘和主机事先约定好： 每次发送的有效数据为1个字节，即8个比特 数据的传输速率（例如1000比特/秒） 键盘在有效数据前面添加起始位（比特0），以通知主机，数据开始发送 键盘在有效数据后面添加停止位（比特1），以通知主机，数据发送结束 下图显示的是：键盘根据和主机的约定，发送给主机的数据和对应的信号。 当按键“k”被按下时，键盘实际上发送的是“起始位”+k对应的ASCII码+“停止位”。 键盘发送信号的过程： 键盘在空闲时，连续地发送正电平给主机，表示当前没有有效数据发送给主机。 当按键“k”被按下时，键盘发送“起始位”+k对应的ASCII码+“停止位”这3者所对应的信号。然后紧跟着继续发送正电平，表示又处于空闲状态。 主机接收信号的过程： 主机开始接收到的一直是正电平，表示当前没有有效数据发送给主机。 主机接收到起始位比特“0”（零电平）后，开始接收比特“0”后面的有效数据。 根据约定，有效数据为1个字节，共8个比特；数据的传输速率例如为1000比特/秒，也就是每毫秒传输1个比特，则8个比特的传输时间为8毫秒。 所以主机根据自己的时钟，在起始位比特“0”后面的8毫秒内接收有效数据。 具体的接收方式（举例说明）是： 主机在起始位后面的0.5毫秒、1.5毫秒、2.5毫秒、3.5毫秒、4.5毫秒、5.5毫秒、6.5毫秒、7.5毫秒分别对信号进行采样，也就是在每个比特的正中间进行采样，以获得这8比特的有效数据。 根据约定，8比特的有效数据后面跟着的是停止位。 如果主机在第8.5毫秒接收到比特“1”（停止位），则主机可以确定数据的发送确实结束了，则结束有效数据的接收，并接受所接收的数据。 如果主机在第8.5毫秒接收到的不是比特“1”（停止位），则主机判断传输过程中发生错误，就放弃所接收到的数据。 无论接受还是放弃所接收到的数据，主机都不会向键盘进行反馈。 键盘向主机发送数据后，就撒手不管了，不会等待主机的确认或其他任何反馈。 在停止位之后，主机接收到的是表示空闲的正电平，继续等待键盘发送数据。]]></content>
      <categories>
        <category>收录文章</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[访问X友软件卡顿]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%BF%E9%97%AEX%E5%8F%8B%E8%BD%AF%E4%BB%B6%E5%8D%A1%E9%A1%BF%2F</url>
    <content type="text"><![CDATA[2017年应用访问卡慢分析1.基本情况：客户环境现象： 总部和分支使用sangfor vpn对接，分支内网电脑ping总部内网服务器，没有丢包，没有延时。但是在分支使用X友客户端访问总部的X友服务器会出现卡顿（即打开客户端后，有些内容显示出来的等待时间较长） 为了排错，客户那边基本没有其他流量在跑。 客户内网拓扑： 总部和分部通过woc做vpn对接（没开加速），其中eth0口是内网口，pppoe和wan（eth2）口是外网口。分支woc的wan口（pppoe）地址是100.64.7.184，eth0（lan口）地址是10.37.11.254，lan口下连一台测试电脑10.37.11.180。总部单臂部署，X友服务器是10.37.1.77。分支pc通过X友软件和总部的X友服务器做数据交互。 2.排错操作： 在分支客户端电脑开始访问X友前，在分支woc和总部woc的eth0口，vpntun口，wan口写下抓包命令。写完之后执行命令，之后再用电脑正式访问X友。看到现象后，等待几秒再停止所有接口的抓包。 其实在抓到上述6份比较漂亮的包之前，我还重复上述操作过几次。虽然时间比较久了，但是我看到自己有导出csv的后缀文件，估计是我那时候想通过比较几个文件中的ip.id来判断访问应用的时候是否存在丢包。估计那时候我没看出啥东西来，就纯浪费时间去了。 后来听说X友这种应用访问卡慢，是因为小包交互过多导致的。就试着打开了一个包fenzhieth0.pcap，然后输入过滤条件，来看下200字节以下的包有多少。emmmm一看占比57.6%呢，小包很多了很多了 于是脑补出理由，打电话给渠道。举个例子，访问X友打开软件要传输15000字节的数据，公网延时是20-30ms，比如小包只传100字节，那这15000字节的数据就要传输150次，时间上就是20ms*150，这样访问就会出现卡顿的情况。如果是用ftp等测试，每次传输是1000字节的话，就是传输15次，所以访问ftp服务器的时候你就不会觉得卡。所以这是X友那边发包机制的问题，和我们设备没有关系 渠道觉得好有道理，然后就信了，之后他去和客户解释就再也没找我了。工单关闭 3.上述排错错在哪 自己想当然的瞎JB乱讲，同时基础概念不清晰，或者说是完全没有概念 在TCP中，发送方和接收方都会存在一个发送窗口和接受窗口。发送窗口表示我发送方一口气能发送多少数据，接收窗口表示我接收方还能接收多少数据放在缓存区中。发送方要尽量保证多发数据，同时也得保证接收方能接收的过来，不至于数据发生溢出。MSS是一个数据段的数据最大长度，那么发送窗口和MSS存在啥关系呢。刚刚说道，发送窗口表示我发送方一口气能发送多少数据，那么MSS的值就确定了，我要一口气发这么多数据要发出多少个包。举个例子来说，我发送方一口气能发送1000个字节，但是一个数据段的MSS是100字节，那么我一口气就能发10个包出去。 给渠道说的理由，看上去好像没问题。其实概念就没弄清。给渠道的说法表明，我的数据包是一个一个发出的，但是实际上TCP中不是这么传输数据的 4.重新整理排错思路 访问一个应用卡慢，分两种情况。一个是网络问题，一个是设备性能问题。 ping测试是在网络层的测试。客户环境中ping测试不丢包不延时，基本上可以判断网络是没啥问题的 之前做了这么一个操作，把pcap文件导出成csv文件，想通过比较各个csv文件中是否存在丢包。但是没有看是什么东西来，而且上述操作比较耗时间。要知道的是，如果数据包丢失，就会导致重传或是有重复ACK。那我们是不是可以通过wireshark工具来自动分析下 先通过过滤条件，把测试电脑与服务器的互访流量给过滤处理 然后在把文件重新保存一份 先打开fenzhieth0这个包。过滤出客户端和服务器双向交互的流量 随机选择一个包，然后右键点击Follow—&gt;tcp stream，过滤出一个tcp连接的互访流量 然后点击统计 statistics—&gt;TCP Stream Graphs—&gt;time sequence （stevens），可以看到该连接，一个方向上的seq号增长情况 可以看到弹出下框，也就意味着。在该连接中，服务器到用户客户端方向的数据流增长过程中，有5s左右是卡住了。那我们通过Stevens图，可以找到卡住的2个点，包号分别是467号包与819号包 可以看到下图，服务器在等待客户端发送816号包。而816号包的发出，是在468号包发出之后耗时近6s。 后续又按上述操作，观察了其他的几个tcp连接，以及zongbueth0口的抓包情况。都是一模一样的，总会有几秒卡住 也就意味着，这是180这台客户端的问题 为什么会出现这种情况，这就是X友厂商应该去分析的了。因为这个是X友应用层层面的问题 5.联想 为什么和我们设备无关了？ ​ ——因为在woc的eth0口抓包，发现上图中468号包发出之后，隔了近6s才收到了816号包。我们设备都没做封装啥的操作呢，只是单纯接收包而已 用woc开启加速后，会有效果吗？ ​ ——个人认为不会，因为woc开启后，是分支的woc设备和分支内网电脑做交互。把woc设备当做是总部的服务器，那内网客户端和woc交互数据的时候，依旧会出现上面这种情况，因为这种情况是在客户端电脑上发生的。要想解决，就得从客户端上看是什么原因导致了这6s的延时发包。]]></content>
      <categories>
        <category>实战</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[英语名词]]></title>
    <url>%2F2019%2F05%2F06%2F%E8%8B%B1%E8%AF%AD%E5%90%8D%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[英语名词1.名词简介名词是人（people）、地（places）、事（things）和想法（ideas） 1.1单复数名词singular and plural nouns单数名词只包含名词本身，只有一个事物 复数名词意味着有更多（more），即一个以上的事物 大部分名词由单数变成复数都是有规律的（即直接在名词后面+s）。但是也有部分名词的变化没有规律 2.名词种类2.1普通名词（common）和专有名词（proper）。普通名词在句首时，首字母才大写。而专有名词的首字母是一直大写 common nouns proper nouns city Chicago frog Kermit river Nile mountain Kilimanjaro 2.2具象名词（concrete）和抽象名词（abstract）。具象表示看得见摸得着的，而抽象名词则相反，比如一些概念。 3.不规则名词复数3.1-f变为-ves singular plural leaf leaves loaf loaves calf calves 3.2以en结尾的复数名词只需要记住一个，child要变成children 3.3单复数同形sheep，fish，bison 3.4突变体复数 singular plural foot feet woman women man men tooth teeth goose geese mouse mice louse lice 3.5外来词复数 语言 变化 单数 复数 规则变化复数 拉丁语 a—ae larva larvae larvas 拉丁语 us—i fungus fungi funguses 拉丁语 um—a datum data 无 拉丁语 ex—ices index indices indexes 希腊语 is—es thesis theses thesises 希腊语 on—a critecion critecia 无]]></content>
      <categories>
        <category>英语</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[软考试题3]]></title>
    <url>%2F2019%2F05%2F06%2F%E8%BD%AF%E8%80%83%E8%AF%95%E9%A2%983%2F</url>
    <content type="text"><![CDATA[软考试题3问题A、B是局域网上两个相距1km的站点，A采用同步传输方式以1Mb/s的速率向B发送长度为200，000字节的文件。假定数据帧长为128比特，其中首部为48比特:应答帧为22比特，A在收到B的应答帧后发送下一帧。传送文件花费的时间为（ ）s，有效的数据速率为（ ）Mb/s（传播速率为200m/us）A.1.6 B.2.4 C.3.2 D.3.6A.0.2 B.0.5 C.0.7 D.0.8 解析 数据传输的过程中，可以理解成是寄快递。比如我想给我妈寄一支牙刷，牙刷用纸盒子包着。我妈拿到这个快递后，真正有用的想要的也就只有这个牙刷而已，那个纸盒子不是真正想要的东西。不过纸盒子上会有寄件人和收件人的信息。 题干中说的假定数据帧长为128比特，其中首部为48比特，其中首部48比特就类似于是纸盒子。128比特就类似于是被纸盒子包着的牙刷。那此时真正有用的数据部分是多少比特呢？聪明如你，很快就知道了是128-48=80bit。也就是说这80bit就类似于是这个牙刷。 现在假设寄的不是牙刷，而是一个需要组装的电脑桌。这个桌子不像牙刷那么好寄，东西太多了，一个纸盒子可能还放不下，要多个纸盒子包着，然后再寄出去。 题干中说的200，000字节（即200，000x8=1600，000比特）的文件就类似于是需要组装的电脑桌。每个纸盒子只能放电脑桌部件的一部分，即每个盒子只能放80比特的内容。 那么此时需要多少个纸盒子装呢？聪明如你，相信已经知道了。1600 000/80=20 000。 一个纸盒子再加上里面的部件，合起来一共是128个比特。那么所有的包裹（纸盒子+电脑桌部件）加起来，占多少个比特呢。就是20 000x128=2560 000比特。 快递公司穷，只有一辆车来运包裹，一次还只能运一个。运过去之后，还得等对方说一句我收到了，才能接着发第二个包裹。对方说的那句我收到了，就类似于题干中说的22比特的应答帧。也就是说，我发20 000个包裹过去，居然要回应20 000次我收到了。 上述了解完后，可以开始做题了。我们知道如何计算一个数据帧从发出到对方接收所需要时间的公式，即发送时延+传播时延 发送时延=数据帧长度/数据速率。把题中数值带入，即128bit/1Mbps=0.000128s。 传播时延=两点间距离/光速的三分之二。把题中数值带入，即1000m/（2x10^8）mps=0.000005s 所以从发送方发出一个帧到接收方接收，所需要的时间是0.000128+0.000005s=0.000133s 我们把包裹发出去，还得等对方说句收到了才能继续发下一个包裹。那么对方说一句收到了，到我们发送方接收到需要多少时间呢？同理可得，发送时延=数据帧长度/数据速率。把题中数值带入，即22bit/1Mbps=0.000022s。传播时延=两点间距离/光速的三分之二。把题中数值带入，即1000m/（2x10^8）mps=0.000005s。所以需要的时间是0.000022s+0.000005s=0.000027s 综上，一来一回所需要的时间就是0.000027s+0.000133s=0.00016s 这还只是一个包需要的时间，现在要发20 000个包，那总共需要多长时间呢？对的，0.00016x20 000=3.2s 最后一问，问你有效数据速率是多少。你的电脑桌就是真正有效的东西，总共也就1600，000比特，为了发这1600000比特，花了一共3.2s的时间。所以有效数据速率就是1600000bit/3.2s=500 000bps=0.5Mbps]]></content>
      <categories>
        <category>软考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[软考试题2]]></title>
    <url>%2F2019%2F05%2F06%2F%E8%BD%AF%E8%80%83%E8%AF%95%E9%A2%982%2F</url>
    <content type="text"><![CDATA[软考试题2问题用64K×8的RAM芯片和32K×16的ROM芯片设计一个256K×16的存储器，地址范围为00000H～3FFFFH，其中ROM的地址范围为10000H～1FFFFH，其余为RAM的地址。则地址线为（）根，数据线为（）根；RAM需要（）片，ROM需要（）片。问题1选项A 18 B 9 C 16 D 8问题2选项A 18 B 9 C 16 D 8问题3选项A 1 B 2 C 3 D 6问题4选项A 12 B 2 C 9 D 6 解析 64Kx8表示什么？ 表示一个芯片的容量。其中64K表示一个芯片内地址块的数目，8表示每个地址块里面存放的比特位数。 其中，1K=1024，1M=1024K。64K=64x1024=65536，即一个芯片内有65536个地址块 你可以理解成，有一个大菜地（芯片），你把这个大菜地分成65536个小菜地（地址块）。每个小菜地里面都能种8颗白菜（比特位数）。一个芯片的容量就是整个大菜地里面能种的白菜数量。 64Kx8与地址线，数据线的关系？ 64K是一个十进制数，即$2^{16}$。 地址线传递地址信息。如果用1个比特，可以表示2个地址块（地址块数目是$2^1$），即编号为0的地址块和编号为1的地址块。如果用2个比特，可以表示4个地址块（地址块数目是$2^2$），即编号为00，01，10，11的四个地址块。那么我有16个比特的话，就能表示64K个地址块。一根地址线就占一个比特位，有16根地址线就表示有16个地址比特位。 数据线传递数据信息。一根数据线就占一个比特位，8根数据线就占8个数据比特位。 这些所谓的线，通过电平的高低变化来表示0或1。只是说不同的线，传递的信息是不同的。地址线传地址信息，数据线传数据信息，控制线传控制信息。这些信息是用0和1表示出来的。 一个256K×16的存储器，由64K×8的RAM芯片和32K×16的ROM芯片组成。 你可以理解成，一个超级大菜地分成2个中型菜地，一个种白菜，一个种辣椒。 这两个中型菜地又分别由小菜地（RAM和ROM芯片）组成。 一个256K×16的存储器，地址范围为00000H～3FFFFH（地址编号为十六进制数）。表示有（3FFFF-00000+1）H个地址块。你想下如果有3个数，编号分别为0~3，是不是一共有4个地址块。用3-0=3是不对的，还得再+1。 即这个存储器（超级大菜地）有（3FFFF-00000+1）H=40000H个地址块，每个地址块能容纳16个比特位数（种16个农作物）。 其中ROM的地址范围为10000H～1FFFFH，这是占了超级大菜地里面的一部分地址块。这部分地址块的数目是多少呢？是（1FFFF-10000+1）H=10000H个。这个十六进制数换成十进制数是64K。 存储器中ROM芯片给的地址块是64K，每个地址块能容纳16个比特。所以存储器中关于ROM的总容量就是64Kx16。 由于题目中说了，存储器是由32K×16的ROM芯片组成。ROM总容量已知，每个ROM芯片的容量也已知，此时问你需要多少片ROM，聪明如你，相信已经知道答案了。（64Kx16）/（32Kx16）=2 ROM占了整个存储器的地址块有64K个。存储器共有256K个。那么留下来给RAM的地址块数目即是（256-64）K=192K个。RAM的总容量就是192Kx16。 知道RAM的总容量，知道每片RAM芯片的容量64K×8，问你需要多少片RAM。聪明如你，相信已经知道答案了。（192Kx16）/（64Kx8）=6]]></content>
      <categories>
        <category>软考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tcpdump技巧]]></title>
    <url>%2F2019%2F04%2F30%2Ftcpdump%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[tcpdump使用技巧1.监视指定网络接口的数据包 [root@www ~]# tcpdump -i eth1 2.也可以指定ip,例如截获所有210.27.48.1 的主机收到的和发出的所有的数据包 [root@www ~]# tcpdump host 210.27.48.1 3.截获主机210.27.48.1 和主机210.27.48.2 或210.27.48.3的通信 [root@www ~]# tcpdump host 210.27.48.1 and \ (210.27.48.2 or 210.27.48.3 \ ) 4.获取主机210.27.48.1除了和主机210.27.48.2之外所有主机通信的ip包 [root@www ~]# tcpdump ip host 210.27.48.1 and ! 210.27.48.2 5.截获主机webserver发送的所有数据 [root@www ~]# tcpdump -i eth0 src host webserver 6.监视所有送到主机webserver的数据包 [root@www ~]# tcpdump -i eth0 dst host webserver 7.获取主机210.27.48.1接收或发出的telnet包 [root@www ~]# tcpdump tcp port 23 and host 210.27.48.1 8.打印所有源地址或目标地址是本地主机的IP数据包 [root@www ~]# tcpdump ip and not net localnet 9.打印长度超过576字节 [root@www ~]# tcpdump ip[2:2] &gt; 576 10.第一个n表示以IP地址的方式显示主机名，第二个N是以端口数字的形式代替服务名。 [root@www ~]# tcpdump -nn [root@www ~]# tcpdump tcp -i eth1 -t -s 0 -c 100 and dst port ! 22 and src net 192.168.1.0/24 -w ./target.cap(1)tcp: ip icmp arp rarp 和 tcp、udp、icmp这些选项等都要放到第一个参数的位置，用来过滤数据报的类型(2)-i eth1 : 只抓经过接口eth1的包(3)-t : 不显示时间戳(4)-s 0 : 抓取数据包时默认抓取长度为68字节。加上-S 0 后可以抓到完整的数据包(5)-c 100 : 只抓取100个数据包(6)dst port ! 22 : 不抓取目标端口是22的数据包(7)src net 192.168.1.0/24 : 数据包的源网络地址为192.168.1.0/24(8)-w ./target.cap : 保存成cap文件，方便用ethereal(即wireshark)分析]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[wireshark技巧]]></title>
    <url>%2F2019%2F04%2F30%2Fwireshark%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[wireshark技巧 资料来源： 1.EMC中文支持论坛 2.如果看了这个你还是不会用Wireshark-那就来找我吧-8月6日完结 3.Linux命令手册-tcpdump 4.Linux上使用wireshark(tshark)抓包分析 5.tcpdump使用技巧 1.抓包前设置过滤条件（捕捉过滤器）尽量避免使用抓包过滤。即便多看几个报文，也比漏看一个报文要好。当你抓取了大量报文的时候，用显示过滤（过滤选项也更多）来重点查看某一数据流。 1.1 抓取指定IP地址的数据流： host 10.3.1.1：抓取发到/来自10.3.1.1的数据流 host 2406:da00:ff00::6b16:f02d：抓取发到/来自IPv6地址2406:da00:ff00::6b16:f02d的数据流 not host 10.3.1.1：抓取除了发到/来自10.3.1.1以外的所有数据流 src host 10.3.1.1：抓取来自10.3.1.1的数据流 dst host 10.3.1.1：抓取发到10.3.1.1的数据流 host 10.3.1.1 or 10.3.1.2：抓取发到/来自10.3.1.1，以及与之通讯的所有数据流，与10.3.1.2，以及与之通讯的所有数据流 host www.espn.com：抓取发到/来自所有解析为www.espn.com的IP地址的数据流 1.2 抓取指定IP地址范围的数据流: net 10.3.0.0/16：抓取网络10.3.0.0上发到/来自所有主机的数据流(16表示长度) net 10.3.0.0 mask 255.255.0.0：与之前的过滤结果相同 ip6 net 2406:da00:ff00::/64：抓取网络2406:da00:ff00:0000(IPv6)上发到/来自所有主机的数据流 not dst net 10.3.0.0/16：抓取除了发到以10.3开头的IP地址以外的所有数据流 not src net 10.3.0.0/16：抓取除了来自以10.3开头的IP地址以外的所有数据流 ip proto &lt; protocol code &gt;：抓取ip协议字段等于&lt; protocol code &gt;值的报文。如TCP(code 6), UDP(code 17), ICMP(code 1)。 ip[2:2]==&lt; number &gt;：ip报文大小 ip[8]==&lt; number &gt;：TTL(Time to Live)值 ip[9]==&lt; number &gt;：协议值 icmp[icmptype]==&lt; identifier &gt;: 抓取 ICMP代码等于identifier的ICMP报文, 如icmp-echo 以及 icmp-request。 方括号中第一个数字表示从协议头开始的偏移量，第二个数字表示需要观察多少位。 1.3 抓取发到广播或多播地址的数据流:只需侦听广播或多播数据流，就可以掌握网络上主机的许多信息。 ip broadcast：抓取广播报文 ip multicast：抓取多播报文 dst host ff02::1：抓取到IPv6多播地址所有主机的数据流 dst host ff02::2：抓取到IPv6多播地址所有路由器的数据流 1.4 抓取基于MAC地址的数据流:当你需要抓取发到/来自某一主机的IPv4或IPv6数据流，可创建基于主机MAC地址的抓包过滤条件。 应用MAC地址时，需确保与目标主机处于同一网段。 ether host 00:08:15:00:08:15：抓取发到/来自00:08:15:00:08:15的数据流 ether src 02:0A:42:23:41:AC：抓取来自02:0A:42:23:41:AC的数据流 ether dst 02:0A:42:23:41:AC：抓取发到02:0A:42:23:41:AC的数据流 not ether host 00:08:15:00:08:15：抓取除了发到/来自00:08:15:00:08:15以外的所有数据流 ether broadcast或ether dst ff:ff:ff:ff:ff:ff：抓取广播报文 ether multicast：多播报文 抓取指定以太网类型的报文：ether proto 0800 抓取指定VLAN：vlan &lt; vlan number &gt; 抓取指定几个VLAN：vlan &lt; vlan number &gt; and vlan &lt; vlan number &gt; 1.5 抓取基于指定应用的数据流:你可能需要查看基于一个或几个应用的数据流。抓包过滤器语法无法识别应用名，因此需要根据端口号来定义应用。通过目标应用的TCP或UDP端口号，将不相关的报文过滤掉。 port 53：抓取发到/来自端口53的UDP/TCP数据流（典型是DNS数据流） not port 53：抓取除了发到/来自端口53以外的UDP/TCP数据流 port 80：抓取发到/来自端口80的UDP/TCP数据流（典型是HTTP数据流） udp port 67：抓取发到/来自端口67的UDP数据流（典型是DHCP据流） tcp port 21：抓取发到/来自端口21的TCP数据流（典型是FTP命令通道） portrange 1-80：抓取发到/来自端口1-80的所有UDP/TCP数据流 tcp portrange 1-80：抓取发到/来自端口1-80的所有TCP数据流 1.6 抓取结合端口的数据流:当你需要抓取多个不连续端口号的数据流，将它们通过逻辑符号连接起来，如下图所示： port 20 or port 21：抓取发到/来自端口20或21的UDP/TCP数据流（典型是FTP数据和命令端口） host 10.3.1.1 and port 80：抓取发到/来自10.3.1.1端口80的数据流 host 10.3.1.1 and not port 80：抓取发到/来自10.3.1.1除了端口80以外的数据流 udp src port 68 and udp dst port 67：抓取从端口68到端口67的所有UDP数据流（典型是从DHCP客户端到DHCP服务器） udp src port 67 and udp dst port 68：抓取从端口67到端口68的所有UDP数据流（典型是从DHCP服务器到DHCP客户端） 抓取TCP连接的开始（SYN）和结束（FIN）报文，配置tcp[tcpflags] &amp; (tcp-syn|tcp-fin)!=0 抓取所有RST(Reset)标志位为1的TCP报文，配置tcp[tcpflags] &amp; (tcp-rst)!=0 less &lt; length &gt;：抓取小于等于某一长度的报文，等同于len &lt;=&lt; length &gt; greater &lt; length &gt;：抓取大于等于某一长度的报文，等同于len &gt;=&lt; length &gt; SYN: 建立连接的信号 FIN: 关闭连接的信号 ACK: 确认接收数据的信号 RST: 立即关闭连接的信号 PSH: 推信号，尽快将数据转由应用处理 tcp[13] &amp; 0x00 = 0: No flags set (null scan) tcp[13] &amp; 0x01 = 1: FIN set and ACK not set tcp[13] &amp; 0x03 = 3: SYN set and FIN set tcp[13] &amp; 0x05 = 5: RST set and FIN set tcp[13] &amp; 0x06 = 6: SYN set and RST set tcp[13] &amp; 0x08 = 8: PSH set and ACK not set tcp[13]是从协议头开始的偏移量，0,1,3,5,6,8是标识位。 2.抓包后设置过滤条件（显示过滤器）2.1 协议过滤器 arp：显示所有包括ARP请求和回复在内的所有ARP数据流。 ip：显示内含IPv4头在内的（如ICMP目的地址不可达报文，在ICMP报文头之后返回到来方向的IPv4头）IP数据流。 ipv6：显示所有IPv6数据流，包括内含IPv6报文头的IPv4报文，如6to4，Teredo，以及ISATAP数据流。 tcp：显示所有基于TCP的数据流。 2.2 应用过滤器 bootp：显示所有DHCP数据流（基于BOOTP）。 dns：显示包括TCP区域传输以及基于标准UDP的DNS请求和回复在内的所有DNS数据流。 tftp：显示所有TFTP（Trivial File Transfer Protocol）数据流。 http：显示所有HTTP命令，回复以及数据传输报文，但不显示TCP握手报文，TCP ACK报文以及TCP结束报文。 icmp：显示所有ICMP报文。 2.3 字符过滤器 tcp.analysis.flags：显示所有包含TCP分析标识的所有报文，包括报文丢失，重传，或零窗口标识。 tcp.analysis.zero_window：显示含有表明发送方的接收缓存用完标识的报文。 2.4 域过滤器 boot.option.hostname：显示所有包含主机名的DHCP数据流（DHCP基于BOOTP）。 http:host：显示所有包含HTTP主机名字段的所有HTTP报文。此报文是客户端向网络服务器发送请求时发出的。 ftp.request.command：显示所有包含命令的FTP数据流，比如USER，PASS，或RETR命令。 2.5 显示过滤器的比较运算符 ==或eq 例如：ip.src == 10.2.2.2 显示所有源地址为10.2.2.2的IPv4数据流 ！=或ne 例如：tcp.srcport != 80 显示源端口除了80以外的所有TCP数据流 gt 或 &gt; 例如：frame.time_relative &gt; 1 显示距前一个报文到达时间相差1秒的报文 &lt;或lt 例如：tcp.window_size &lt; 1460 显示当TCP接收窗口小于1460字节时的报文 ge 或 &gt;= 例如：dns.count.answers &gt;= 10 显示包含10个以上answer的DNS响应报文 &lt;=或le 例如：ip.ttl &lt;= 10 显示IP报文中Time to Live字段小于等于10的报文 contains 例如：http contains “GET” 显示所有HTTP客户端发送给HTTP服务器的GET请求 对于基于TCP应用的过滤条件采用比较运算符。例如，如果想看端口80上面的HTTP数据流，使用HTTP.port==80。 小贴士： 运算符两边不用留空格。ip.src == 10.2.2.2与ip.src==10.2.2.2的效果是相同的。 2.6 举例应用2.6.1 按照某一IP地址或主机过滤报文： 例如：ip.addr==10.3.1.1 显示在IP源地址字段或IP目的地址字段包含10.3.1.1的帧。 例如：！ip.addr==10.3.1.1 显示除了在IP源地址字段或IP目的地址字段包含10.3.1.1以外的帧。 例如：ipv6.addr==2406:da00:ff00::6b16:f02d 显示以2406:da00:ff00::6b16:f02d为源地址或目的地址的帧。 例如：ip.src==10.3.1.1 显示所有来自10.3.1.1的数据流。 例如：ip.dst==10.3.1.1 显示所有发往10.3.1.1的数据流 例如：ip.host==www.wireshark.org 显示所有解析为www.wireshark.org的IP 2.6.2 按照某一IP地址范围过滤报文：可以使用&gt;或&lt;比较运算符或逻辑运算符&amp;&amp;查找某一地址范围内的报文。 例如：ip.addr&gt;10.3.0.1&amp;&amp;ip.addr&lt;10.3.0.5 显示来自或发往10.3.0.2，10.3.0.3，10.3.0.4的数据流。 例如：(ip.addr&gt;=10.3.0.1&amp;&amp;ip.addr&lt;=10.3.0.6)&amp;&amp;!ip.addr==10.3.0.3 显示来自或发往10.3.0.1，10.3.0.2，10.3.0.4，10.3.0.5，10.3.0.6的数据流，10.3.0.3排除在外。 例如：ipv6.addr&gt;=fe80::&amp;&amp;ipv6.addr&lt;fec0:: 显示IPv6地址从0xfe80到0xfec0开头的数据流。 2.6.3 按照某一IP子网过滤报文：可以通过ip.addr字段名定义一个子网。这种格式使用IP地址后跟斜杠以及一个后缀，表明IP地址中定义的网络部分的比特数。 例如：ip.addr==10.3.0.0/16 显示在IP源地址或目的地址字段以10.3开头的数据流。 例如：ip.addr == 10.3.0.0/16 &amp;&amp; ！ip.addr==10.3.1.1 显示除了10.3.1.1以外，在IP源地址或目的地址字段以10.3开头的数据流。 例如：!ip.addr == 10.3.0.0/16 &amp;&amp; !ip.addr==10.2.0.0/16 显示所有数据流，除了在IP源地址或目的地址字段以10.3和10.2开头的数据流 注意： 错误的用法导致不work： 错误：ip.addr != 10.2.2.2 显示在IP源地址或IP目的地址不包含10.2.2.2的报文。只要在源或目的IP地址不为10.2.2.2，报文就会被显示出来。这时隐含的或会导致实际上并未过滤任何报文。 正确：！ip.addr == 10.2.2.2 显示IP源地址和IP目的地址都不包含10.2.2.2的报文。 错误：!tcp.flags.syn==1 显示所有不含TCP SYN bit设置为1的报文。其他协议报文，必须UDP和ARP报文也符合这一过滤条件，因为它们的TCP SYN bit没有设置为1。 正确：tcp.flags.syn！=1 只显示包含SYN设置为0的TCP报文。]]></content>
      <categories>
        <category>小技巧</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[文件系统的简单操作]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[文件系统的简单操作1.df和du命令df列出文件系统的整体磁盘使用量，df读取的数据几乎针对一整个文件系统，因为读取范围是超级区块内的信息。 123456789101112131415161718192021[root@vultr ~]# dfFilesystem 1K-blocks Used Available Use% Mounted ondevtmpfs 214728 0 214728 0% /devtmpfs 246392 0 246392 0% /dev/shm/dev/vda1 10291200 2231176 7519412 23% /# Filesystem表示文件系统在哪个磁盘分区# 1K-blocks表示单位为1k，可以利用-h来变成人们易于理解的单位格式# Mounted on表示挂载点--------------------------------------------------------------[root@vultr ~]# df -hFilesystem Size Used Avail Use% Mounted ondevtmpfs 210M 0 210M 0% /devtmpfs 241M 0 241M 0% /sys/fs/cgroup/dev/vda1 9.9G 2.2G 7.2G 23% /--------------------------------------------------------------[root@vultr ~]# df -ThFilesystem Type Size Used Avail Use% Mounted ondevtmpfs devtmpfs 210M 0 210M 0% /devtmpfs tmpfs 241M 0 241M 0% /sys/fs/cgroup/dev/vda1 ext4 9.9G 2.2G 7.2G 23% /# Type表示文件系统类型 dudu不同于df，du会在整个文件系统内去查找所有的文件数据 2.硬件链接和软链接（符号链接）2.1硬件链接目录的数据区块下新增一条文件名链接到某个inode号码的关联记录。即多个文件名对应到同一个inode号码。 硬链接的限制：1.不能跨文件系统。2.不能连接目录 123456789[root@vultr tmp]# ll -itotal 4 11618 -rw-r--r-- 1 root root 0 Apr 18 06:16 test[root@vultr tmp]# [root@vultr tmp]# ln test zhang[root@vultr tmp]# ll -itotal 4 11618 -rw-r--r-- 2 root root 0 Apr 18 06:16 test 11618 -rw-r--r-- 2 root root 0 Apr 18 06:16 zhang 2.2符号链接符号链接就是建立一个独立的文件，而这个文件会让数据的读取指向它连接的那个文件的文件名 当源文件被删除后，符号链接的文件就会打不开。类似于是Windows下的快捷方式 需要注意的是，通过符号链接进入到了某个目录或是打开了某个文件。操作的对象实际上还是原始文件。所以你在符号链接里面打开了东西，进行了操作，那原始文件也会发生改变。如果你在符号链接里面把内容删除了，相当于原始文件的实际内容也删除了。 123456[root@vultr tmp]# ln -s test zhangshuaiyang 加上-s就是符号链接，不加是硬链接[root@vultr tmp]# ll -itotal 4 11618 -rw-r--r-- 2 root root 0 Apr 18 06:16 test 11618 -rw-r--r-- 2 root root 0 Apr 18 06:16 zhang 23193 lrwxrwxrwx 1 root root 4 Apr 18 06:43 zhangshuaiyang -&gt; test 3.磁盘分区、格式化、检验及挂载如果想在系统里面新增一块硬盘，操作过程如下： 对磁盘进行划分，进行磁盘分区 对分区进行格式化，以建立文件系统 对文件系统进行检验（可选） 在linux系统上，建立一个挂载点（目录），把该文件系统挂载上来 3.1查看磁盘分区状态lsblk（list block device）显示出所有存储设备 12345678910111213141516[root@vultr tmp]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsr0 11:0 1 1024M 0 rom vda 253:0 0 10G 0 disk └─vda1 253:1 0 10G 0 part /#name表示设备文件名，会省略/dev等前面的目录；disk表示一整块磁盘；part表示分区#在vda这个磁盘下，有一个分区vda1----------------------------------------------------------[root@vultr tmp]# lsblk -ipf 通过-f选项可以列出UUIDNAME FSTYPE LABEL UUID MOUNTPOINT/dev/sr0 /dev/vda `-/dev/vda1 ext4 19c82273-828c-4d85-bc4e-6fc269acc524 /-----------------------------------------------------------[root@vultr tmp]# blkid 通过blkid也可以显示出UUID/dev/vda1: UUID="19c82273-828c-4d85-bc4e-6fc269acc524" TYPE="ext4" parted列出磁盘的分区表类型及分区信息 123456789[root@vultr tmp]# parted /dev/vda printModel: Virtio Block Device (virtblk)Disk /dev/vda: 10.7GBSector size (logical/physical): 512B/512BPartition Table: msdos #分区表格式，比如MBR/GPT，这里显示的是msdosDisk Flags: #下面内容是分区数据Number Start End Size Type File system Flags 1 1049kB 10.7GB 10.7GB primary ext4 boot 3.2磁盘分区MBR分区表使用fdisk工具进行，GPT分区表使用gdisk工具进行。 3.3磁盘格式化所谓格式化，其实是指在分区上创建文件系统。使用的命令是mkfs（make filesystem） 如果我们要创建的文件系统是xfs。那么命令的写法就是mkfs.xfs。例如mkfs.xfs /dev/vda4，表示在/dev/vda4这个分区上建立一个xfs文件系统。 如果是创建ext4文件系统。则命令写法是mkfs.ext4。只是xfs的文件系统建立起来速度更快。 想知道能创建哪些文件系统。只要再命令行中输入mkfs[tab][tab]，就能看到了。 3.4文件系统的挂载注意点： 单一文件系统不要被重复挂载到不同的挂载点中 单一目录不要重复挂载多个文件系统 作为挂载点的目录，最好是空目录 12345[root@vultr tmp]# blkid /dev/vda1: UUID="19c82273-828c-4d85-bc4e-6fc269acc524" TYPE="ext4" [root@vultr tmp]# mount UUID="19c82273-828c-4d85-bc4e-6fc269acc524" /data/ext4其中UUID表示文件系统的ID。/data/ext4表示挂载点。也就是说把这个文件系统挂载在/data/ext4目录下。]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[文件系统]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[文件系统柱面通常是分区及文件系统的最小单位。当然如果使用GPT格式的分区表，最小单位可以为扇区 磁盘分区完后，要进行格式化，使之成为操作系统能用的文件系统 文件（或目录）包含属性（比如文件权限，文件属性）及内容 权限与属性 实际数据 inode表 数据区块 区块有放置inode表的区块，有放置数据的数据区块，以及记录整个文件系统整体信息的超级区块 超级区块：记录inode和数据区块总量、使用量、剩余量及文件系统格式等 inode：记录文件属性，一个文件占一个inode，同时记录文件对应数据所放置的区块号码 数据区块：记录实际内容，当一个区块占不下的时候，就会占用多个数据区块 索引式文件系统，包含了inode。通过inode，我们可以数据存放在哪些数据区块里面，所以可以一次性全部提取出来。该方式性能比较好，比如linux的ext2文件系统 再比如u盘一般使用FAT文件系统，这种就不是索引式文件系统。它没有inode，所以无法把数据一次性全提取出来。每个区块号码都记录在前一个区块里面，所以提取的时候得一个连着一个提取。 碎片整理，在非索引式文件系统里面，由于有时候数据写入的区块太过于分散，所以读取的时候性能会很差，通过磁盘碎片整理，可以把同一个文件的区块集合在一起，这样读取起来就比较容易。 1.ext2文件系统在分区上进行格式化的时候，就已经规划好了inode和数据区块 把放置inode的区块和数据区块全部都放一起，会导致很难管理 所以在ext2的文件系统上，会弄出多个区块群组，每个区块群组都有自己的inode，数据区块和超级区块 在文件系统最前面有个启动扇区，里面可以存放启动引导程序。（注：或是想到之前的磁盘分区，分区的第一个扇区用来放MBR，MBR中的446字节来存放启动引导程序） 这种设计可以把引导启动程序放在不同文件系统的最前端，而不必把程序全都放在整个磁盘唯一的MBR下。 1.1数据区块数据区块用来放置文件数据。ext2文件系统支持的区块大小有1k，2k及4k共3种。 每个区块都有编号，方便inode表记录。 数据放在数据区块里面，如果数据的大小要小于区块的容量的话，就会造成浪费。 1.2inode表inode表记录如下内容 该文件的读写属性（读、写、执行） 文件所有者及用户组 文件大小 文件建立或状态改变时间 最近一次读取时间 最近修改时间 定义文件特征标识 文件真正内容指向 inode表的数量和大小在格式化的时候就确定好了 每个inode表大小为128B（其中4B来记录一个数据区块的位置编号，一共能记录12个） 每个文件仅占用一个inode表 文件系统能建立的文件数量和inode数量有关 系统读取文件先找到inode表，分析文件记录用户及权限是否符合，如果符合最后才去找数据区块读取 1.3超级区块没有超级区块，就没有文件系统，它记录整个文件系统相关信息。 数据区块与inode的总量 未使用与已经使用的inode与数据区块的数量 数据区块与inode的大小 文件系统的挂载时间、最近一次写入数据时间等文件系统相关信息 一个有效位数值，文件系统被挂载，则有效位为0，未被挂载则为1 123# 显示目前系统被格式化的设备[root@vultr ~]# blkid/dev/vda1: UUID="19c82273-828c-4d85-bc4e-6fc269acc524" TYPE="ext4" 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667[root@vultr ~]# dumpe2fs /dev/vda1dumpe2fs 1.42.9 (28-Dec-2013)Filesystem volume name: &lt;none&gt;Last mounted on: /Filesystem UUID: 19c82273-828c-4d85-bc4e-6fc269acc524Filesystem magic number: 0xEF53Filesystem revision #: 1 (dynamic)Filesystem features: has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isizeFilesystem flags: signed_directory_hash Default mount options: user_xattr aclFilesystem state: clean #文件系统状态，clean表示没问题Errors behavior: ContinueFilesystem OS type: LinuxInode count: 640000 #inode总数Block count: 2621179 #区块总数Reserved block count: 131057 #保留的区块总数Free blocks: 2026205 #还有多少可用区块Free inodes: 599345 #还有多少可用inodeFirst block: 0Block size: 4096 #单个区块大小Fragment size: 4096Group descriptor size: 64Reserved GDT blocks: 248Blocks per group: 32768Fragments per group: 32768Inodes per group: 8000Inode blocks per group: 500Flex block group size: 16Filesystem created: Wed Dec 5 17:22:49 2018Last mount time: Wed Apr 10 10:10:48 2019Last write time: Wed Apr 10 10:10:46 2019Mount count: 4Maximum mount count: -1Last checked: Sat Mar 9 02:38:28 2019Check interval: 0 (&lt;none&gt;)Lifetime writes: 3452 MBReserved blocks uid: 0 (user root)Reserved blocks gid: 0 (group root)First inode: 11Inode size: 256 #inode表大小Required extra isize: 28Desired extra isize: 28Journal inode: 8Default directory hash: half_md4Directory Hash Seed: 6cbf0c8a-63c4-43df-aaba-cd82ae9cb153Journal backup: inode blocksJournal features: journal_incompat_revoke journal_64bitJournal size: 32MJournal length: 8192Journal sequence: 0x0004f118Journal start: 7195Group 0: (Blocks 0-32767) [ITABLE_ZEROED] #区块组0所占区块号码为0-32767 Checksum 0x6f25, unused inodes 0 Primary superblock at 0, Group descriptors at 1-2 #文件系统描述说明在1-2号区块内 Reserved GDT blocks at 3-250 Block bitmap at 251 (+251), Inode bitmap at 267 (+267) #区块对照表和inode对照表在251和267区块内 Inode table at 283-782 (+283) #一个inode表占256B，总共有782-283+1=500个区块，一个区块大小占4KB。所以inode的总数是500*4kB/256B #以下几行记录，记录了可用的区块数，及可用的inode表述。可知，inode表为0了。 19990 free blocks, 0 free inodes, 1728 directories Free blocks: 11866, 11890-11922, 11928-11935, 11943-11951, 12249, 12313, 12350, 12832-32767 Free inodes: ……后续省略…… 2.文件系统与目录树的关系2.1目录在文件系统上创建一个目录时，文件系统会分配一个inode与至少一块区块（里面记录了子目录及文件的inode表位置编号）给目录 2.2文件在文件系统上创建一个文件时，文件系统会分配一个inode及所需个数的数据区块。 2.3目录树的读取文件存放在目录下，所以我们得要先知道该目录所对应的区块。区块里面记录了子目录及文件的inode表号。找到我想要的文件所对应的inode表号，再去找该文件的inode。找到之后就知道该文件放置在哪个区块了。 3.挂载文件系统和目录树结合的操作称为挂载 挂载点一定是目录，该目录为进入文件系统的入口 文件系统要挂载在目录树的某个目录后，我们才能使用该文件系统 4.XFS文件系统ext文件系统的支持度虽然广，但是格式化所需要时间长。因为是预先就规划好了inode和区块的位置，所以后续可以直接使用，也就是说并没有使用动态配置的做法 现在的磁盘越来越大，对于虚拟化磁盘那就更大了。对于巨型文件要考虑性能问题，不然虚拟磁盘的处理效率就会差。 xfs是一个日志式文件系统，用于高容量磁盘及高性能文件。同时还几乎有ext4文件系统有的功能。此外，inode与区块都是需要时才动态配置产生，不会预分配，所以格式化操作会很快。 4.1数据区包含inode、数据区块、超级区块等数据，类似于之前ext里面的区块群组 4.2文件系统活动登陆区用来记录文件系统的变化，有点像日志区 4.3实时运行区当有文件要被建立时，xfs会在这个区段里找一个到数个的扩展区块，将文件放置在这个区块内，等分配完毕再写入到数据区的inode与区块中。]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[磁盘分区]]></title>
    <url>%2F2019%2F04%2F30%2F%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[磁盘分区1.磁盘分区磁盘包含：碟片（细分为扇区和磁道）、机械手臂、磁头、主轴马达 扇区大小分为512B和4KB 1.1第一个扇区（MBR）磁盘第一个扇区存放启动引导程序和磁盘分区表（格式分为MBR[Windows支持]和GPT） MBR（master boot record） 启动引导程序 磁盘分区表 主引导记录容量（512B） 446B 64B MBR分区表的限制： 分区最多到2TB MBR仅占一个扇区，被破坏后，很难恢复甚至无法恢复 MBR内的启动引导程序只有446B，无法存储较多的程序代码 1.1.1MBR磁盘分区表 分区表占64B，给磁盘划分分区，其实就是对这个分区表做设置 分区表默认最多把一个磁盘分为4个分区 分区分为主要分区和扩展分区 要写入数据到磁盘时，会参考这个分区表 1.1.2MBR扩展分区由于磁盘分区表的限制，最多给磁盘划分出4个分区。那么我们可以利用额外的扇区来记录更多的分区信息。 在扩展分区的某个地方来记录在扩展分区里面的其他逻辑分区信息 扩展分区最多一个 逻辑分区是在扩展分区里面划分出来的 能格式化的是主要分区和逻辑分区，扩展分区不能被格式化 逻辑分区能划分多少个，依据不同操作系统来定 1.2GPT磁盘分区表现在的磁盘越来越大，如果使用磁盘阵列等技术，那么在Linux平台下看到的磁盘大小可能就有几十个TB。使用MBR格式，要划分分区时，就要2TB/2TB地划分下去。这就可能划分出好几十个分区，为了解决这个问题，就有了GPT这种磁盘分区的格式。 以前扇区大小为512B，现在已经有了4KB的扇区。为兼容所有磁盘，会使用到逻辑区块地址LBA（logical block address），LBA默认是512B。在GPT这种格式下，将磁盘的所有区块使用LBA来规划。可以理解为，如果一个扇区的容量是512B，然后LBA默认是512B的话，那么一个扇区就是一个区块。如果一个扇区容量是4KB=8*512B，那么一个区块地址就是1/8扇区 第一个LBA称为LBA0 MBR使用第一个扇区来记录，而GPT使用了前34个LBA区块来记录。由于MBR只有一个区块，破坏就难以恢复。不同于MBR，GPT中会用磁盘的最后34个LBA做备份。 1.2.1LBA0（MBR兼容区块） LBA0 引导启动程序 特殊标志符 逻辑区块地址0 446B 64B，表示磁盘使用GPT格式 1.2.2LBA1（GPT表头记录）记录磁盘分区表本身的位置和大小，同时记录了备份的GPT分区位置。 1.2.3LBA2-33（实际记录分区信息处）从LBA2区块开始，每个LBA可以记录4组分区记录。所以一个磁盘在默认情况下，可以划分4*32=128个分区。 在MBR中，分区表的大小是64B，而在GPT中是512B。 一个区块记录4组分区，所以在GPT的分区表中，一组分区记录可以占512/4=128B的空间大小。 在这128B的空间中使用64bit来记录开始和结束的扇区号码 每个分区的最大容量限制就是：$2^{64}*512B=8ZB$ 2.启动程序CMOS是一个嵌入在主板的存储器，存储着各项硬件参数 BIOS是一个写入到主板的固件（固件是写入到硬件上的一个软件程序） 计算机系统在启动时，主动执行的第一个程序就是BIOS BIOS执行后，会分析计算机里面的存储设备。比如发现了硬盘，BIOS就去找那个能启动的硬盘 找到该硬盘，就读取第一个扇区的MBR位置，找到那个446B的启动引导程序 以上BIOS的任务完成，接下来就是启动引导程序干活了 启动引导程序的目的是加载内核文件 加载完后，启动引导程序的活也干完了 之后就是内核文件开始工作，再之后就是把任务都交给我们熟悉的操作系统完成]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[软考试题1]]></title>
    <url>%2F2019%2F04%2F29%2F%E8%BD%AF%E8%80%83%E8%AF%95%E9%A2%981%2F</url>
    <content type="text"><![CDATA[问题若主机hostA的MAC地址为aa-aa-aa-aa-aa-aa，主机hostB的MAC地址为bb-bb-bb-bb-bb-bb。由hostA发出的查询hostB的MAC地址的帧格式如下图所示，则此帧中的目标MAC地址为（ D ），ARP报文中的目标MAC地址为（ C ）。 问题1选项 A aa-aa-aa-aa-aa-aa B bb-bb-bb-bb-bb-bb C 00-00-00-00-00-00 D ff-ff-ff-ff-ff-ff 问题2选项 A aa-aa-aa-aa-aa-aa B bb-bb-bb-bb-bb-bb C 00-00-00-00-00-00 D ff-ff-ff-ff-ff-ff 解析 数据包从上往下层层封装。在wireshark里面是可以看到其封装情况的。 上图就是一个arp的数据包。在中间的框中，可以知道arp报文被Ethernet（以太网）头部封装。第一个包指的是arp广播请求包，意思是“谁知道192.168.0.1的mac地址，请告诉192.168.0.114“ wireshark数据包中以太网帧头部（Ethernet）包含了destination（目标mac地址），source（源mac地址），type（类型） 问题一里面的目的mac指的就是以太网头部里面的mac地址。 即下图中写的目标mac地址，你能看到正好就是对应wireshark数据包中的destination这段。 问题二里面，ARP报文中的目标MAC地址，指的就是wireshark数据包中，红框的target mac address。arp报文是被mac头部封装的。 sender mac address表示发送方自己的mac地址； sender ip address表示发送方自己的ip地址； target mac address表示想要知道的对方的mac地址是多少，由于是广播请求，此时并不知道对方mac，所以暂时以全0替代； target ip address表示请求对方的ip地址。]]></content>
      <categories>
        <category>软考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设置分类列表页]]></title>
    <url>%2F2019%2F04%2F27%2F%E8%AE%BE%E7%BD%AE%E5%88%86%E7%B1%BB%E5%88%97%E8%A1%A8%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[设置分类列表页问题最近通过github+hexo(ocean主题)搭建了博客。期间遇到了各种问题，主要还是因为自己不是程序员，有些代码看不懂。 比如我想，以后如果文章多起来，只有一个归档可不行，最好要有个分类的界面。把文章都整理好，以后也方便自己查东西。 照着网上的操作： 先生成一个分类的网页，输入命令hexo new page categories 找到对应网页的md文件，在其头部内添加上type: &quot;categories&quot; 最后测试，新建一篇文章，在文章md文件头部上添加categories: 分类的名称 保存 最后需要在主页上把分类页展现出来。 找到主题配置文件_config.yml。在meun中加入分类: /categories 保存 上述操作做完之后，查看效果。发现文章里面是有分类的显示 但是，在主页里面点击分类的连接，进入分类网页，没有任何显示。如下图红框处，最开始是没有任何显示的. 解决方法网上查了大量文章，很多都只是说了配置问题，我是按配置正确操作的，但是就是解决不了。 后来看到一篇文章，猜测出应该是这个ocean主题缺少对应的代码。 参考链接：hexo主题开发指南-分类列表页与分类文章列表页 分类列表页显示博客里的所有分类，分类文章列表页显示某个分类中的文章列表。 Hexo 并没有专门分类列表页的模板，那该如何处理呢？一般是写在页面模板中，即 layout/page.swig 里，然后判断页面类型变量 page.type，如果是 categories，则显示分类列表页。再在博客里创建一个页面，指定其 type 为 categories ocean主题是用ejs写出来的。而这个链接里面给的是swig的代码。所以估计不能直接用 定位了问题，接下来就好处理了。由于我暂时还不会写代码，所以只好找答案复制粘贴。 参考链接：Hexo-创建分类（categories）和标签（tags）首页，给出如下代码 找到 layout/_partial/article.ejs 然后找到 &lt;div class=&quot;article-entry&quot; itemprop=&quot;articleBody&quot;&gt; 这一行 这个 div 里面的内容全部替换为：(注：再此处，我没有全部替换，而是补充接在原始文档内容后了) 123456789101112131415161718192021222324252627282930313233343536373839&gt; &lt;% if (page.type === "tags") &#123; %&gt;&gt; &lt;div class="tag-cloud"&gt;&gt; &lt;div class="tag-cloud-title"&gt;&gt; &lt;%- _p('counter.tag_cloud', site.tags.length) %&gt;&gt; &lt;/div&gt;&gt; &gt; &lt;div class="tag-cloud-tags"&gt;&gt; &lt;%- tagcloud(&#123;&gt; min_font: 12,&gt; max_font: 30,&gt; amount: 200,&gt; color: true,&gt; start_color: '#ccc',&gt; end_color: '#111'&gt; &#125;) %&gt;&gt; &lt;/div&gt;&gt; &lt;/div&gt;&gt; &gt; &lt;% &#125; else if (page.type === 'categories') &#123; %&gt;&gt; &gt; &lt;div class="category-all-page"&gt;&gt; &lt;div class="category-all-title"&gt;&gt; &lt;%- _p('counter.categories', site.categories.length) %&gt;&gt; &lt;/div&gt;&gt; &gt; &lt;div class="category-all"&gt;&gt; &lt;%- list_categories() %&gt;&gt; &lt;/div&gt;&gt; &gt; &lt;/div&gt;&gt; &gt; &lt;% &#125; else &#123; %&gt;&gt; &gt; &lt;% if (post.excerpt &amp;&amp; index)&#123; %&gt; &lt;%- post.excerpt %&gt;&gt; &lt;% &#125; else &#123; %&gt;&gt; &lt;%- post.content %&gt;&gt; &lt;% &#125; %&gt;&gt; &lt;% &#125; %&gt;&gt; 修改样式，如果觉得不好看，自己改喜欢的样式 找到 yilia/source/css/_partial/article.styl 在最后面添加下面的 css 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107&gt; /*tag-cloud*/&gt; .tag-cloud &#123;&gt; text-align: center;&gt; margin-top: 50px;&gt; &#125;&gt; .tag-cloud a &#123;&gt; display: inline-block;&gt; margin: 10px;&gt; &#125;&gt; .tag-cloud-title &#123;&gt; font-weight: 700;&gt; font-size: 24px;&gt; &#125;&gt; .tag-cloud-tags &#123;&gt; margin-top: 15px;&gt; a &#123;&gt; display: inline-block;&gt; text-decoration: none;&gt; font-weight: normal;&gt; font-size: 10px;&gt; color: #fff;&gt; line-height: normal;&gt; padding: 5px 5px 5px 10px;&gt; position: relative;&gt; border-radius: 0 5px 5px 0;&gt; font-family: Menlo, Monaco, "Andale Mono", "lucida console", "Courier New", monospace;&gt; &amp;:hover &#123;&gt; opacity: 0.8;&gt; &#125;&gt; &amp;:before &#123;&gt; content: " ";&gt; width: 0;&gt; height: 0;&gt; position: absolute;&gt; top: 0;&gt; left: -18px;&gt; border: 9px solid transparent;&gt; &#125;&gt; &amp;:after &#123;&gt; content: " ";&gt; width: 4px;&gt; height: 4px;&gt; background-color: #fff;&gt; border-radius: 4px;&gt; box-shadow: 0 0 0 1px rgba(0, 0, 0, .3);&gt; position: absolute;&gt; top: 7px;&gt; left: 2px;&gt; &#125;&gt; &#125;&gt; a.color1 &#123;&gt; background: #FF945C;&gt; &amp;:before &#123;&gt; border-right-color: #FF945C;&gt; &#125;&gt; &#125;&gt; a.color2 &#123;&gt; background: #F5C7B7;&gt; &amp;:before &#123;&gt; border-right-color: #F5C7B7;&gt; &#125;&gt; &#125;&gt; a.color3 &#123;&gt; background: #BA8F6C;&gt; &amp;:before &#123;&gt; border-right-color: #BA8F6C;&gt; &#125;&gt; &#125;&gt; a.color4 &#123;&gt; background: #CFB7C4;&gt; &amp;:before &#123;&gt; border-right-color: #CFB7C4;&gt; &#125;&gt; &#125;&gt; a.color5 &#123;&gt; background: #7B5D5F;&gt; &amp;:before &#123;&gt; border-right-color: #7B5D5F;&gt; &#125;&gt; &#125;&gt; &#125;&gt; &gt; /*category-all-page*/&gt; .category-all-page &#123;&gt; margin-top: 50px;&gt; .category-all-title &#123;&gt; font-weight: 700;&gt; font-size: 24px;&gt; text-align: center;&gt; &#125;&gt; .category-list-item:after &#123;&gt; content: '';&gt; clear: both;&gt; display: table;&gt; &#125;&gt; .category-list-count &#123;&gt; float: right;&gt; margin-left: 5px;&gt; &#125;&gt; .category-list-count:before &#123;&gt; content: '一共 ';&gt; &#125;&gt; .category-list-count:after &#123;&gt; content: ' 篇文章';&gt; &#125;&gt; &#125;&gt; 通过上述操作，问题解决。不过显示出来的样式不是自己喜欢的那种。等以后自己学会编程，再回头过来修改吧～]]></content>
      <categories>
        <category>小技巧</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tshark使用方法]]></title>
    <url>%2F2019%2F04%2F26%2Ftshark%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[tshark使用方法tshark官方文档 1.介绍TShark is a network protocol analyzer. It lets you capture packet data from a live network, or read packets from a previously saved capture file, either printing a decoded form of those packets to the standard output or writing the packets to a file. TShark‘s native capture file format is pcapng format, which is also the format used by wireshark and various other tools. TShark是一个网络分析工具。它能帮你在实时网络中捕获数据包，或是从预先保存好的捕获文件中读取数据包，或是打印出这些数据包的解码形式到标准输出，再或是把数据包写入到一个文件中。TShark的本地捕获文件格式是pcapng格式，这种pcapng格式也被wireshark和多种其他工具使用。 Without any options set, TShark will work much like tcpdump. It will use the pcap library to capture traffic from the first available network interface and displays a summary line on the standard output for each received packet. 如果没有设置任何选项，TShark将像tcpdump一样工作。它使用pcap库，从第一个可使用的网络接口捕获流量。并且为每个接收到的包展示其摘要行到标准输出上。 When run with the -r option, specifying a capture file from which to read, TShark will again work much like tcpdump, reading packets from the file and displaying a summary line on the standard output for each packet read. TShark is able to detect, read and write the same capture files that are supported by Wireshark. The input file doesn’t need a specific filename extension; the file format and an optional gzip compression will be automatically detected. Near the beginning of the DESCRIPTION section of wireshark(1) or https://www.wireshark.org/docs/man-pages/wireshark.html is a detailed description of the way Wireshark handles this, which is the same way Tshark handles this. 当使用-r选项，会从我们指定的文件中读取数据包信息。TShark将再次像tcpdump一样工作，从文件中读取数据包并且把读取的数据包在标准输出上展示其摘要行。TShark可以检测，读取和写入同一份捕获文件，这些操作在Wireshark中也是支持的。输出文件不需要一个指定的文件扩展名；它将动态检测文件格式和可选的gzip压缩。在Wireshark的开始描述部分附近或是在链接https://www.wireshark.org/docs/man-pages/wireshark.html 中，介绍了关于Wireshark处理这些问题的方法细节描述，这些方法同样适用于TShark。 Compressed file support uses (and therefore requires) the zlib library. If the zlib library is not present when compiling TShark, it will be possible to compile it, but the resulting program will be unable to read compressed files. 支持压缩文件要使用（因此需要）zlib库。如果编译TShark时zlib库不存在，也可以编译它，但是最终程序将不可读取压缩文件。 When displaying packets on the standard output, TShark writes, by default, a summary line containing the fields specified by the preferences file (which are also the fields displayed in the packet list pane in Wireshark), although if it’s writing packets as it captures them, rather than writing packets from a saved capture file, it won’t show the “frame number” field. If the -V option is specified, it instead writes a view of the details of the packet, showing all the fields of all protocols in the packet. If the -O option is specified, it will only show the full details for the protocols specified, and show only the top-level detail line for all other protocols. Use the output of “tshark -G protocols“ to find the abbreviations of the protocols you can specify. If the -P option is specified with either the -V or -O options, both the summary line for the entire packet and the details will be displayed. 当在标准输出显示数据包时，默认情况下TShark输出摘要行信息，摘要行里包含首选项文件指定的字段（这些字段也展示在wireshark中的包列表窗），但是如果在捕获流量时输出数据包而不是在保存的文件中输出数据包的话，将不会显示“帧编号”字段。如果指定了-V选项，这将输出数据包的细节信息视图，展示了数据包中所有协议的所有字段信息。如果指定了-O选项，它将仅显示指定协议的完整详细信息，并仅显示所有其他协议的顶级详细信息行。在命令行中输入“tshark -G protocols”可以查找指定的协议缩写。如果-P选项和-V或-O一起使用，将会展示整个包的摘要行和细节信息。 Packet capturing is performed with the pcap library. That library supports specifying a filter expression; packets that don’t match that filter are discarded. The -f option is used to specify a capture filter. The syntax of a capture filter is defined by the pcap library; this syntax is different from the read filter syntax described below, and the filtering mechanism is limited in its abilities. 数据包捕获时使用pcap库。pcap库支持指定的过滤表达式；数据包没有匹配上过滤表达式则会被丢弃。-f选项被用来指定捕获过滤表达式。捕获过滤的语法在pcap库中定义；这些捕获过滤的语法不同于以下所描述的显示过滤器语法，并且其过滤机制的能力有限。 Read filters in TShark, which allow you to select which packets are to be decoded or written to a file, are very powerful; more fields are filterable in TShark than in other protocol analyzers, and the syntax you can use to create your filters is richer. As TShark progresses, expect more and more protocol fields to be allowed in read filters. Read filters use the same syntax as display and color filters in Wireshark; a read filter is specified with the -R option. 在TShark中的显示过滤器允许你选择哪一个包被解码或是把该数据包写入到一个文件，这是很强大的功能；TShark相比于其他协议分析器可以过滤出更多的字段，并且你能使用并创建的过滤器语法更为丰富。随着TShark的发展，期待更多协议字段被允许出现在显示过滤器中。显示过滤器使用与wireshark中的展示和色彩过滤器一样的语法；使用-R选项来指定显示过滤器。 Read filters can be specified when capturing or when reading from a capture file. Note that that capture filters are much more efficient than read filters, and it may be more difficult for TShark to keep up with a busy network if a read filter is specified for a live capture, so you might be more likely to lose packets if you’re using a read filter. 当正在捕获或是从一个捕获文件中读取时是可以指定显示过滤器的。需要注意的是捕获过滤器比显示过滤器会更有效率；并且在一个繁忙的网络中如果进行实时捕获时使用了显示过滤器，那么TShark可能更难跟上这个繁忙网络，同时你要是使用了显示过滤器还可能会丢失数据包。 A capture or read filter can either be specified with the -f or -R option, respectively, in which case the entire filter expression must be specified as a single argument (which means that if it contains spaces, it must be quoted), or can be specified with command-line arguments after the option arguments, in which case all the arguments after the filter arguments are treated as a filter expression. If the filter is specified with command-line arguments after the option arguments, it’s a capture filter if a capture is being done (i.e., if no -r option was specified) and a read filter if a capture file is being read (i.e., if a -r option was specified). 捕获或是显示过滤器能分别使用-f或是-R选项来指定。在这种情况下，整个过滤表达式必须作为一个参数被指定（这意味着如果含有空格，就需要使用“ ”被引用）；或者是在选项参数之后使用命令行参数被指定，在这种情况下，所有在过滤器参数之后的参数会被视为过滤表达式。如果在选项参数后，使用命令行参数来指定过滤器，那么捕获正在进行时它就是捕获过滤器（即，没有-r选项）；如果捕获文件正在被读取，那么它就是显示过滤器（即，-r选项是被指定的）。 If the -w option is specified when capturing packets or reading from a capture file, TShark does not display packets on the standard output. Instead, it writes the packets to a capture file with the name specified by the -w option. 当正在捕获数据包，或是从一个捕获文件中读取时，如果使用了-w选项，那么TShark不会在标准输出上显示数据包。相反，它将把数据包写入捕获文件，其名称由-w选项指定。 If you want to write the decoded form of packets to a file, run TShark without the -w option, and redirect its standard output to the file (do not use the -w option). 如果要将解码后的数据包形式写入文件，那么使用TShark时不要带上-w选项，同时会将其标准输出重定向到文件。（不要使用-w选项） If you want the packets to be displayed to the standard output and also saved to a file, specify the -P option in addition to the -w option to have the summary line displayed, specify the -V option in addition to the -w option to have the details of the packet displayed, and specify the -O option, with a list of protocols, to have the full details of the specified protocols and the top-level detail line for all other protocols to be displayed. If the -P option is used together with the -V or -O option, the summary line will be displayed along with the detail lines. 如果你想数据包在标准输出上显示并且还能保存到一个文件中，那么除了-w选项还需要指定-P选项来显示摘要行。使用-w选项及-V选项将展示数据包的细节。如果再加上-O选项，带上了列出的协议，将显示指定协议的所有细节以及所有其他协议的顶层细节行。如果-P选项和-V或是-O选项一起使用，那么摘要行将会和细节信息一起展示。 When writing packets to a file, TShark, by default, writes the file in pcapng format, and writes all of the packets it sees to the output file. The -F option can be used to specify the format in which to write the file. This list of available file formats is displayed by the -F option without a value. However, you can’t specify a file format for a live capture. 当把数据包写入一个文件，TShark默认情况下会使用pcapng格式，并将其所有看到的包写入到输出文件。使用-F选项可以指定输出文件的格式。使用-F选项不带任何参数值，将显示可以得到的文件格式列表。但是对于实时捕获，你不能指定其文件格式。 When capturing packets, TShark writes to the standard error an initial line listing the interfaces from which packets are being captured and, if packet information isn’t being displayed to the terminal, writes a continuous count of packets captured to the standard output. If the -q option is specified, neither the continuous count nor the packet information will be displayed; instead, at the end of the capture, a count of packets captured will be displayed. If the -Q option is specified, neither the initial line, nor the packet information, nor any packet counts will be displayed. If the -q or -Q option is used, the -P, -V, or -O option can be used to cause the corresponding output to be displayed even though other output is suppressed. 当正在捕获数据包时，TShark把捕获到数据包接口的初始化行写入到标准错误中。如果数据包信息没有被展示在终端，则将写入连续的捕获数据包统计到标准输出。如果-q选项被指定，则不管是连续统计还是数据包信息都不会被展示出来；相反，在捕获结束后，被捕获的数据包统计将会显示出来。如果-Q选项被指定，初始化行、数据包信息或是任何一个数据包统计都不会被展示。如果使用-q或-Q选项，则可以使用-P，-V或-O选项来显示相应的输出，即使其他输出被抑制也是如此。 When reading packets, the -q and -Q option will suppress the display of the packet summary or details; this would be used if -z options are specified in order to display statistics, so that only the statistics, not the packet information, is displayed. 读取数据包时，-q和-Q选项将禁止显示数据包摘要或详细信息;如果指定-z选项以显示统计信息，那么只有统计信息会被展示，而不会展示数据包信息。 The -G option is a special mode that simply causes Tshark to dump one of several types of internal glossaries and then exit. -G选项是一种特殊模式，它只会导致TShark转储几种类型的内部词汇表中的一种，然后退出。 2.选项概要Capture interface: 123456789-i &lt;interface&gt; # name or idx of interface (def: first non-loopback)-f &lt;capture filter&gt; # packet filter in libpcap filter syntax-s &lt;snaplen&gt; # packet snapshot length (def: 262144)-p # don't capture in promiscuous mode-I # capture in monitor mode, if available-B &lt;buffer size&gt; # size of kernel buffer (def: 4MB)-y &lt;link type&gt; # link layer type (def: first appropriate)-D # print list of interfaces and exit-L # print list of link-layer types of iface and exit Capture stop conditions: 12345-c &lt;packet count&gt; # stop after n packets (def: infinite)-a &lt;autostop cond.&gt; ... ​ duration:NUM - stop after NUM seconds​ filesize:NUM - stop this file after NUM KB​ files:NUM - stop after NUM files Capture output: 12345-b &lt;ringbuffer opt.&gt; ... ​ duration:NUM - switch to next file after NUM secs​ filesize:NUM - switch to next file after NUM KB​ files:NUM - ringbuffer: replace after NUM files Input file: 1-r &lt;infile&gt; # set the filename to read from (no stdin!) Processing: 12345678910-2 # perform a two-pass analysis-R &lt;read filter&gt; # packet Read filter in Wireshark display filter syntax-Y &lt;display filter&gt; # packet displaY filter in Wireshark display filter syntax-n # disable all name resolutions (def: all enabled)-N &lt;name resolve flags&gt; # enable specific name resolution(s): "mnNtC"-d &lt;layer_type&gt;== &lt;selector&gt;,&lt;decode_as_protocol&gt; ... # "Decode As", see the man page for details # Example: tcp.port==8888,http-H &lt;hosts file&gt; ​ read a list of entries from a hosts file, which will then be written to a capture file. (Implies -W n) Output: 1234567891011121314151617181920212223242526272829303132333435-w &lt;outfile|-&gt; # write packets to a pcap-format file named "outfile" # (or to the standard output for "-") -C &lt;config profile&gt; # start with specified configuration profile-F &lt;output file type&gt; # set the output file type, default is pcapng # an empty "-F" option will list the file types -V # add output of packet tree (Packet Details)-O &lt;protocols&gt; # Only show packet details of these protocols, comma separated-P # print packet summary even when writing to a file-S &lt;separator&gt; # the line separator to print between packets-x # add output of hex and ASCII dump (Packet Bytes)-T pdml|ps|psml|text|fields # format of text output (def: text)-e &lt;field&gt; # field to print if -Tfields selected (e.g. tcp.port, col.Info);this option can be repeated to print multiple fields-E&lt;fieldsoption&gt;=&lt;value&gt; # set options for output when -Tfields selected: header=y|n switch headers on and off separator=/t|/s|&lt;char&gt; select tab, space, printable character as separator occurrence=f|l|a print first, last or all occurrences of each field aggregator=,|/s|&lt;char&gt; select comma, space, printable character as aggregator quote=d|s|n select double, single, no quotes for values -t a|ad|d|dd|e|r|u|ud # output format of time stamps (def: r: rel. to first)-u s|hms # output format of seconds (def: s: seconds)-l # flush standard output after each packet-q # be more quiet on stdout (e.g. when using statistics)-Q # only log true errors to stderr (quieter than -q)-g # enable group read access on the output file(s)-W n # Save extra information in the file, if supported. # n = write network address resolution information -X &lt;key&gt;:&lt;value&gt; # eXtension options, see the man page for details-z &lt;statistics&gt; # various statistics, see the man page for details Miscellaneous: 1234567-h # display this help and exit-v # display version info and exit-o &lt;name&gt;:&lt;value&gt; ... # override preference setting-K &lt;keytab&gt; # keytab file to use for kerberos decryption-G [report] # dump one of several available reports and exit # default report="fields" # use "-G ?" for more help 3.选项细节3.1 Capture interface:捕获接口-i 指定接口1-i &lt;interface&gt; # name or idx of interface (def: first non-loopback) Set the name of the network interface or pipe to use for live packet capture. 为实时数据包捕获设置网络接口或管道的名称。 Network interface names should match one of the names listed in “tshark -D“ (described above); a number, as reported by “tshark -D“, can also be used. If you’re using UNIX, “netstat -i“, “ifconfig -a“ or “ip link“ might also work to list interface names, although not all versions of UNIX support the -a option to ifconfig. 网络接口名称应该是使用“tshark -D”命令后显示的接口名称列表中的一个。当然也可以使用“tshark -D”展示的列表中的数字。如果你使用UNIX系统，“netstat -i”，“ifconfig -a”或是“iplink”也可以显示出接口名称，尽管不是所有的NUIX版本都支持在ifconfig中使用-a参数。 If no interface is specified, TShark searches the list of interfaces, choosing the first non-loopback interface if there are any non-loopback interfaces, and choosing the first loopback interface if there are no non-loopback interfaces. If there are no interfaces at all, TShark reports an error and doesn’t start the capture. 如果没有接口被指定，TShark寻找接口列表，若在列表中存在多个非回环接口，将选择第一个非回环接口。若在列表中没有非回环接口，则选择第一个回环接口。如果设备没有一个接口，那TShark会报告一个错误，并且不会开始捕获数据。 Pipe names should be either the name of a FIFO (named pipe) or “-“ to read data from the standard input. On Windows systems, pipe names must be of the form “\\pipe\.\pipename”. Data read from pipes must be in standard pcapng or pcap format. Pcapng data must have the same endianness as the capturing host. 管道名称（此处略） This option can occur multiple times. When capturing from multiple interfaces, the capture file will be saved in pcapng format. 这个选项可以出现多次。当从多个接口进行数据捕获，捕获文件将被保存为pcapng格式。 -f 设置捕获时的过滤条件1-f &lt;capture filter&gt; # packet filter in libpcap filter syntax Set the capture filter expression.设置捕获过滤器表达式 This option can occur multiple times. If used before the first occurrence of the -i option, it sets the default capture filter expression. If used after an -i option, it sets the capture filter expression for the interface specified by the last -i option occurring before this option. If the capture filter expression is not set specifically, the default capture filter expression is used if provided. 这个选项可以多次出现。如果在第一次出现-i选项之前使用，则会设置默认的捕获过滤器表达式。如果在-i选项之后使用，则会为最后一个-i选项指定的接口设置捕获过滤器表达式。如果捕获过滤器表达式没有设置指定，则使用默认的捕获过滤表达式（如果提供的话） Pre-defined capture filter names, as shown in the GUI menu item Capture-&gt;Capture Filters, can be used by prefixing the argument with “predef:”. Example: tshark -f “predef:MyPredefinedHostOnlyFilter” 通过在参数前面添加前缀”predef:”可以使用预定义捕获过滤器名称，就像在GUI菜单选项 Capture-&gt;Capture Filters中一样。举个例子：tshark -f “predef:MyPredefinedHostOnlyFilter” ps：捕获过滤器条件写法参考自己之前做的总结文档 -s 设置捕获的数据包长度1-s &lt;snaplen&gt; # packet snapshot length (def: 262144) Set the default snapshot length to use when capturing live data. No more than snaplen bytes of each network packet will be read into memory, or saved to disk. A value of 0 specifies a snapshot length of 262144, so that the full packet is captured; this is the default. 当在捕获实时数据时，设置一个默认的快照长度。每个网络数据包的快照字节将会读入到内存或是保存在硬盘中。 数值0指定了快照长度是262144字节，以便捕获完整数据包；这个也是默认的。 This option can occur multiple times. If used before the first occurrence of the -i option, it sets the default snapshot length. If used after an -i option, it sets the snapshot length for the interface specified by the last -i option occurring before this option. If the snapshot length is not set specifically, the default snapshot length is used if provided. 这个选项能出现多次。如果在第一次出现-i选项前使用，则将设置默认的快照长度。如果在-i选项后使用，则将为最后一个出现的-i选项所指定的接口设置快照长度。如果快照长度没有被指定，则使用默认的快照长度（如果被提供的话） -p 设置接口为非混杂模式1-p # don't capture in promiscuous mode Don’t put the interface into promiscuous mode. Note that the interface might be in promiscuous mode for some other reason; hence, -p cannot be used to ensure that the only traffic that is captured is traffic sent to or from the machine on which TShark is running, broadcast traffic, and multicast traffic to addresses received by that machine. 不让接口成为混杂模式。 This option can occur multiple times. If used before the first occurrence of the -i option, no interface will be put into the promiscuous mode. If used after an -i option, the interface specified by the last -i option occurring before this option will not be put into the promiscuous mode. 这个选项能出现多次。如果在第一次出现-i选项前使用，那么没有接口会被设置为混杂模式。如果在-i选项后被使用，那么在-p选项前的最后一个-i选项指定的接口将不会被设置为混杂模式。 -I 为IEEE802.11设置监控模式1-I # capture in monitor mode, if available Put the interface in “monitor mode”; this is supported only on IEEE 802.11 Wi-Fi interfaces, and supported only on some operating systems. 设置为监控模式；这仅在IEEE802.11 Wi-Fi接口和某些操作系统上支持。 -B 设置捕获缓冲区大小1-B &lt;buffer size&gt; # size of kernel buffer (def: 4MB) Set capture buffer size (in MiB, default is 2 MiB). 设置捕获缓冲区大小，TSshark官方文档中说默认是2MB。在我这台Linux服务器显示是4MB -y 设置数据链路类型1-y &lt;link type&gt; # link layer type (def: first appropriate) Set the data link type to use while capturing packets. The values reported by -L are the values that can be used. 当在捕获数据包时，设置数据链路类型。能够使用的值在-L参数中被展示 This option can occur multiple times. If used before the first occurrence of the -i option, it sets the default capture link type. If used after an -i option, it sets the capture link type for the interface specified by the last -i option occurring before this option. If the capture link type is not set specifically, the default capture link type is used if provided. -D 输出接口列表1-D # print list of interfaces and exit -L 显示数据链路类型1-L # print list of link-layer types of iface and exit 3.2 Capture stop conditions:捕获停止选项-c 在N个数据包后停止捕获1-c &lt;packet count&gt; # stop after n packets (def: infinite) Set the maximum number of packets to read when capturing live data. If reading a capture file, set the maximum number of packets to read. 在捕获实时数据时，设置一个最大的数据包读取数。如果是在读取捕获文件，依旧是要设置一个读取数据包的数量。第一张图是实时捕获的情况，第二张图是读取http_google.pcap文件只看前3个包的情况。 -a 设置停止捕获条件12345-a &lt;autostop cond.&gt; ... ​ duration:NUM - stop after NUM seconds​ filesize:NUM - stop this file after NUM KB​ files:NUM - stop after NUM files Specify a criterion that specifies when TShark is to stop writing to a capture file. The criterion is of the form test:value, where test is one of: 指定一个标准，指定TShark何时停止写入捕获文件。标准的写法是test：value，其中test是以下之一 duration:value Stop writing to a capture file after value seconds have elapsed. Floating point values (e.g. 0.5) are allowed. duration:value 经过value秒后停止捕获文件。duration是持续时间的意思。浮点数值也是被允许的（比如0.5）。下图测试，确实在1s后停止捕获，同时还告诉你6个包被捕获到了 files:value Stop writing to capture files after value number of files were written. files:value 在捕获value个文件后，就停止捕获 filesize:value Stop writing to a capture file after it reaches a size of value kB. If this option is used together with the -b option, TShark will stop writing to the current capture file and switch to the next one if filesize is reached. When reading a capture file, TShark will stop reading the file after the number of bytes read exceeds this number (the complete packet will be read, so more bytes than this number may be read). Note that the filesize is limited to a maximum value of 2 GiB. filesize:value 在达到value kB的大小后停止写入捕获文件。如果此选项与-b选项一起使用，则TShark将停止写入当前捕获文件，并在达到文件大小时切换到下一个文件。读取捕获文件时，TShark将在读取的字节数超过此数字后停止读取该文件（因为要读取完整数据包，所以可能会读取出超出这个数值的字节数）。请注意，文件大小限制为2 GiB。 3.3 Capture output:捕获输出-b 设置循环写入多个数据包条件1234-b &lt;ringbuffer opt.&gt; ... ​ duration:NUM - switch to next file after NUM secs​ filesize:NUM - switch to next file after NUM KB​ files:NUM - ringbuffer: replace after NUM files Cause TShark to run in “multiple files” mode. In “multiple files” mode, TShark will write to several capture files. When the first capture file fills up, TShark will switch writing to the next file and so on. 导致TShark使用“多文件”模式运行。在“多文件”模式下，TShark将写入多个捕获文件。当第一个捕获文件写满，TShark将切换写入到下一个文件，以此类推。 The created filenames are based on the filename given with the -w option, the number of the file and on the creation date and time, e.g. outfile_00001_20190714120117.pcap, outfile_00002_20190714120523.pcap, … 被创建的这些文件名是基于-w选项所给出的“文件名，文件的编号以及在创建时的数据和时间”。例如 outfile_00001_20190714120117.pcap，outfile_00002_20190714120523.pcap, … With the files option it’s also possible to form a “ring buffer”. This will fill up new files until the number of files specified, at which point TShark will discard the data in the first file and start writing to that file and so on. If the files option is not set, new files filled up until one of the capture stop conditions match (or until the disk is full). 使用files选项还可以形成“环形缓冲区”。这将填充新文件直到所指定的文件数。在这一点上，TShark将丢弃在第一个文件中的数据并开始把数据写入另一个文件中，以此类推。如果files选项没有被设置，则将填满新文件，直到其中一个捕获停止条件匹配为止（或是直到硬盘被填满）。 The criterion is of the form key:value, where key is one of: 标准格式是key:value，key是以下参数中的一个： duration:value switch to the next file after value seconds have elapsed, even if the current file is not completely filled up. Floating point values (e.g. 0.5) are allowed. duration:value 在经过value秒后切换到下一个文件，即便现在的文件没有被完全填满。浮点数值（例如0.5）也是可以使用的。 files:value begin again with the first file after value number of files were written (form a ring buffer). This value must be less than 100000. Caution should be used when using large numbers of files: some filesystems do not handle many files in a single directory well. The files criterion requires either duration, interval or filesize to be specified to control when to go to the next file. It should be noted that each -b parameter takes exactly one criterion; to specify two criterion, each must be preceded by the -b option. files:value 在value个文件数被写入后，再次从第一个文件开始（形成环形缓冲区）。这个数值必须小于100000。当使用大量的文件数时，需要谨慎使用：因为一些文件系统不能在当个的目录下处理好大量的文件。文件标准要求指定持续时间，间隔或文件大小以控制何时转到下一个文件。需要注意的是每个-b参数只使用一个标准；想要使用两个标准，那么每个标准前都要加上-b参数 filesize:value switch to the next file after it reaches a size of value kB. Note that the filesize is limited to a maximum value of 2 GiB. filesize:value 在到达value kB后，切换到下一个文件。需要注意的是文件大小被限制在2GB以下。 3.4 Input file:读取本地文件-r1-r &lt;infile&gt; # set the filename to read from (no stdin!) 3.5 Processing:处理过程-2 执行2次分析1-2 # perform a two-pass analysis 执行两次分析 -R 设置显示过滤器1-R &lt;read filter&gt; # packet Read filter in Wireshark display filter syntax Cause the specified filter (which uses the syntax of read/display filters, rather than that of capture filters) to be applied during the first pass of analysis. Packets not matching the filter are not considered for future passes. Only makes sense with multiple passes, see -2. For regular filtering on single-pass dissect see -Y instead. 在第一遍分析中使用指定过滤器（该过滤器使用的是读取/显示过滤器的语法，而不是捕获过滤器的语法）。没有匹配过滤器的数据包将不会在后续展示。-R对于数据包进行多次分析才有意义，可以参考-2。对于单次的常规分析详见-Y。 -Y 设置显示过滤器（单次分析）1-Y &lt;display filter&gt; # packet displaY filter in Wireshark display filter syntax Cause the specified filter (which uses the syntax of read/display filters, rather than that of capture filters) to be applied before printing a decoded form of packets or writing packets to a file. Packets matching the filter are printed or written to file; packets that the matching packets depend upon (e.g., fragments), are not printed but are written to file; packets not matching the filter nor depended upon are discarded rather than being printed or written. 在输出数据包的解码形式或写入数据包到文件前，使用指定的过滤器（过滤器使用读取/显示过滤器的语法，而不是捕获过滤器）。匹配过滤的数据包将输出或是写入到文件；基于匹配的数据包（例如：数据段），将不会输出但是会写入到文件；不匹配过滤器的数据包被丢弃而不是被打印或写入。 Use this instead of -R for filtering using single-pass analysis. If doing two-pass analysis (see -2) then only packets matching the read filter (if there is one) will be checked against this filter. 对于过滤时使用-R将进行单次分析。如果做2次分析（见-2），那么只有数据包匹配了读取过滤器（如果有的话），将会针对这个过滤器再次检查。 -n 设置不做名称解析1-n # disable all name resolutions (def: all enabled) Disable network object name resolution (such as hostname, TCP and UDP port names); the -N option might override this one. 关闭所有名称解析（例如主机名，TCP和UDP的端口名）；-N选项将覆盖-n选项。 -N 设置只为特定情况做名称解析1-N &lt;name resolve flags&gt; # enable specific name resolution(s): "mnNtC" Turn on name resolving only for particular types of addresses and port numbers, with name resolving for other types of addresses and port numbers turned off. This option overrides -n if both -N and -n are present. If both -N and -n options are not present, all name resolutions are turned on. 只为特定的地址和端口号类型打开名称解析，对于没有指定的则不进行地址解析。如果-N和-n选项同时出现，则会覆盖-n选项。如果-N和-n选项都没出现，则名称解析会被开启。 The argument is a string that may contain the letters: 参数是包含以下字母的字符串 d to enable resolution from captured DNS packets 从捕获的DNS数据包中开始解析 m to enable MAC address resolution 开启MAC地址解析 n to enable network address resolution 开启网络地址解析 N to enable using external resolvers (e.g., DNS) for network address resolution 使用外部解析器（如DNS）来对网络地址进行解析 t to enable transport-layer port number resolution 开启传输层端口号解析 v to enable VLAN IDs to names resolution 开启VLAN ID的名称解析 -d 设置解码格式123-d &lt;layer_type&gt;== &lt;selector&gt;,&lt;decode_as_protocol&gt; ... # "Decode As", see the man page for details # Example: tcp.port==8888,http Like Wireshark’s Decode As… feature, this lets you specify how a layer type should be dissected. If the layer type in question (for example, tcp.port or udp.port for a TCP or UDP port number) has the specified selector value, packets should be dissected as the specified protocol. 类似于Wireshark的Decode As…这个功能让你指定如何对每层类型进行分析。如果请求的层次类型（如TCP或是UDP端口号中的tcp.port 、udp.port ）有指定的选择值，则数据包将会使用指定的协议进行分析。 Example: tshark -d tcp.port==8888,http will decode any traffic running over TCP port 8888 as HTTP. 例如：tshark -d tcp.port==8888,http将会解码每一个TCP端口号是8888的流量为HTTP协议。 Example: tshark -d tcp.port==8888:3,http will decode any traffic running over TCP ports 8888, 8889 or 8890 as HTTP. 例如：tshark -d tcp.port==8888:3,http将会解码每一个TCP端口号是8888，8889,8890的流量为HTTP协议。 Example: tshark -d tcp.port==8888-8890,http will decode any traffic running over TCP ports 8888, 8889 or 8890 as HTTP. 含义同上，只是写法有区别 Using an invalid selector or protocol will print out a list of valid selectors and protocol names, respectively. 使用无效的解析器或协议将会分别打印出有效解析器和协议名称的表。 Example: tshark -d . is a quick way to get a list of valid selectors. 例如：tshark -d .以最快的方式列出有效解析器 Example: tshark -d ethertype==0x0800. is a quick way to get a list of protocols that can be selected with an ethertype. 例如：tshark -d ethertype==0x0800.以最快的方式获取能选择的以太网类型的协议列表 -H12-H &lt;hosts file&gt; ​ read a list of entries from a hosts file, which will then be written to a capture file. (Implies -W n) Read a list of entries from a “hosts” file, which will then be written to a capture file. Implies -W n. Can be called multiple times. 从“hosts”文件中读取条目列表，然后将其写入捕获文件。也可以使用-W n。可以多次调用。 3.6 Output:输出-w 写入文件12-w &lt;outfile|-&gt; # write packets to a pcap-format file named "outfile" # (or to the standard output for "-") Write raw packet data to outfile or to the standard output if outfile is ‘-‘. 在标准输出或是输出文件中写入原始数据包 NOTE: -w provides raw packet data, not text. If you want text output you need to redirect stdout (e.g. using ‘&gt;’), don’t use the -w option for this. 注意：-w提供的是原始数据包，而非文本。如果你想输出文本，你需要重定向（例如使用‘&gt;’），而非使用-w选项。 -F 设置写入文件格式12-F &lt;output file type&gt; # set the output file type, default is pcapng # an empty "-F" option will list the file types Set the file format of the output capture file written using the -w option. The output written with the -w option is raw packet data, not text, so there is no -F option to request text output. The option -F without a value will list the available formats. 使用-w选项设置输出捕获文件写入的文件格式。使用-w选项写入的输出是原始数据包而非文本。-F选项后不接值，则将列出可获得的格式。 -V（大写V，显示数据包细节）1-V # add output of packet tree (Packet Details) Cause TShark to print a view of the packet details. -O 显示此选项指定的协议的详细信息1-O &lt;protocols&gt; # Only show packet details of these protocols, comma separated Similar to the -V option, but causes TShark to only show a detailed view of the comma-separated list of protocols specified, and show only the top-level detail line for all other protocols, rather than a detailed view of all protocols. Use the output of “tshark -G protocols“ to find the abbreviations of the protocols you can specify. 类似于-V，只是TShark仅显示指定协议的以逗号分隔开的协议细节。同时仅为其他的协议显示顶层细节行，而非全体细节。使用 “tshark -G protocols“ 你可以获得可以指定的协议列表。 -T 与-e一起使用，显示相应的特定内容1-T pdml|ps|psml|text|fields # format of text output (def: text) Set the format of the output when viewing decoded packet data. The options are one of: 当在查看解码数据包数据时设置输出格式。选项可以为以下之一（我只列出常用的）： fields The values of fields specified with the -e option, in a form specified by the -E option. For example, fields 使用-e选项指定字段的值，采用-E选项指定格式。例如， 1tshark -T fields -E separator=, -E quote=d -e1-e &lt;field&gt; # field to print if -Tfields selected (e.g. tcp.port, col.Info);this option can be repeated to print multiple fields Add a field to the list of fields to display if -T ek|fields|json|pdml is selected. This option can be used multiple times on the command line. At least one field must be provided if the -T fields option is selected. 如果选择了-T ek|fields|json|pdml，则添加一个字段来显示字段列表。在命令行，这个选项可以使用多次。如果使用的是 -T fields ，则至少要提供一个字段。 Example: *tshark -e frame.number -e ip.addr -e udp * 例如：*tshark -e frame.number -e ip.addr -e udp * Giving a protocol rather than a single field will print multiple items of data about the protocol as a single field. Fields are separated by tab characters by default. -E controls the format of the printed fields. 给定一个协议而不是单个字段，将会打印出这个协议作为单个字段的多个项目数据。字段默认情况下会被制表符分隔。-E控制打印字段的格式。 -E 设置控制字段打印的选项123456-E&lt;fieldsoption&gt;=&lt;value&gt; # set options for output when -Tfields selected: header=y|n switch headers on and off separator=/t|/s|&lt;char&gt; select tab, space, printable character as separator occurrence=f|l|a print first, last or all occurrences of each field aggregator=,|/s|&lt;char&gt; select comma, space, printable character as aggregator quote=d|s|n select double, single, no quotes for values -t 设置时间显示格式1-t a|ad|d|dd|e|r|u|ud # output format of time stamps (def: r: rel. to first) -u 设置秒的类型1-u s|hms # output format of seconds (def: s: seconds) Specifies the seconds type. Valid choices are: s for seconds hms for hours, minutes and seconds -W n12-W n # Save extra information in the file, if supported. # n = write network address resolution information Save extra information in the file if the format supports it. For example, 如果格式支持，则保存额外的信息到文件中。例如，使用如下命令 1tshark -F pcapng -W n will save host name resolution records along with captured packets. 则将保存主机名称解析的解码到捕获数据包中。 Future versions of Tshark may automatically change the capture format to pcapng as needed. 未来版本的Tshark可能会根据需要自动将捕获格式更改为pcapng。 The argument is a string that may contain the following letter: n write network address resolution information (pcapng only) 参数可以是以下的字符串： n 写入网络地址解析信息(仅限pcapng) -q与-z 获取各种统计信息（只挑选了部分常用的）12-q # be more quiet on stdout (e.g. when using statistics)-z &lt;statistics&gt; # various statistics, see the man page for details Get TShark to collect various types of statistics and display the result after finishing reading the capture file. Use the -q option if you’re reading a capture file and only want the statistics printed, not any per-packet information. 让TShark收集统计的各种类型，并且在完成读取的捕获文件后展示出来。如果你正在读取捕获文件，并只想打印出统计信息，而不是每个包的信息，则使用-q选项。 Note that the -z proto option is different - it doesn’t cause statistics to be gathered and printed when the capture is complete, it modifies the regular packet summary output to include the values of fields specified with the option. Therefore you must not use the -q option, as that option would suppress the printing of the regular packet summary output, and must also not use the -V option, as that would cause packet detail information rather than packet summary information to be printed. 注意的是-z proto有不同之处。当在捕获完成时，它不会收集统计信息并打印出来，而是修改常规数据包汇总输出，包括选项中指定字段的数值。因此，你不能使用-q选项，因为该选项会禁止打印常规数据包的汇总输出。同时，也不能使用-V选项，因为这会导致数据包的细节信息被打印出来，而非数据包的汇总信息。 Currently implemented statistics are:目前实现的统计有如下内容： -z help Display all possible values for -z. 显示所有可能的数值 -z afp,srt[,filter] Show Apple Filing Protocol service response time statistics. 显示AFP服务器响应时间统计 -z conv,type[,filter] Create a table that lists all conversations that could be seen in the capture. type specifies the conversation endpoint types for which we want to generate the statistics; currently the supported ones are: 创建一个表，里面包含了所有统计信息，这些信息可以在捕获中看到。type指定了我们想要生成的统计信息的会话终端类型。目前支持的内容如下： 12345678910111213141516"bluetooth" Bluetooth addresses"eth" Ethernet addresses"fc" Fibre Channel addresses"fddi" FDDI addresses"ip" IPv4 addresses"ipv6" IPv6 addresses"ipx" IPX addresses"jxta" JXTA message addresses"ncp" NCP connections"rsvp" RSVP connections"sctp" SCTP addresses"tcp" TCP/IP socket pairs Both IPv4 and IPv6 are supported"tr" Token Ring addresses"usb" USB addresses"udp" UDP/IP socket pairs Both IPv4 and IPv6 are supported"wlan" IEEE 802.11 addresses If the optional filter is specified, only those packets that match the filter will be used in the calculations. 如果指定了filter，则在计算时会使用匹配了过滤条件的这些数据包。 The table is presented with one line for each conversation and displays the number of packets/bytes in each direction as well as the total number of packets/bytes. The table is sorted according to the total number of frames. 这个表会为每个会话显示一行，并且显示每个方向的数据包/字节的数目及其总数。该表会依据数据帧数进行排序。 -z bootp,stat[,filter] Show DHCP (BOOTP) statistics. 显示DHCP统计信息 -z dns,tree[,filter] Create a summary of the captured DNS packets. General information are collected such as qtype and qclass distribution. For some data (as qname length or DNS payload) max, min and average values are also displayed. 为捕获的DNS数据包创建一个汇总。核心信息包含了如qtype，qclass distribution。对于一些数据（如qname长度或是DNS负载）的最大，最小，平均值也会被显示出来。 -z endpoints,type[,filter] Create a table that lists all endpoints that could be seen in the capture. type specifies the endpoint types for which we want to generate the statistics; currently the supported ones are: 12345678910111213141516"bluetooth" Bluetooth addresses"eth" Ethernet addresses"fc" Fibre Channel addresses"fddi" FDDI addresses"ip" IPv4 addresses"ipv6" IPv6 addresses"ipx" IPX addresses"jxta" JXTA message addresses"ncp" NCP connections"rsvp" RSVP connections"sctp" SCTP addresses"tcp" TCP/IP socket pairs Both IPv4 and IPv6 are supported"tr" Token Ring addresses"usb" USB addresses"udp" UDP/IP socket pairs Both IPv4 and IPv6 are supported"wlan" IEEE 802.11 addresses If the optional filter is specified, only those packets that match the filter will be used in the calculations（计算）.The table is presented with one line for each conversation and displays the number of packets/bytes in each direction as well as the total number of packets/bytes. The table is sorted according to the total number of frames. 与-z conv,type[,filter]类似 -z expert [,error|,warn|,note|,chat|,comment*][,filter*] Collects information about all expert info, and will display them in order, grouped by severity. Example: -z expert,sip will show expert items of all severity for frames that match the sip protocol. This option can be used multiple times on the command line. 收集所有的expert info专家信息，并按顺序及重要性分组来显示他们。 例如: -z expert,sip 将显示与sip协议匹配的帧的所有重要专家项。 If the optional filter is provided, the stats will only be calculated on those calls that match that filter. Example: -z “expert,note,tcp” will only collect expert items for frames that include the tcp protocol, with a severity of note or higher. 如果提供了filter，则只会根据与该过滤器匹配的调用计算统计信息。 例如：-z “expert,note,tcp” 将仅收集note或更高层级的包含了tcp协议的数据帧的专家项 -z flow,name,mode,[filter] Displays the flow of data between two nodes. Output is the same as ASCII format saved from GUI. name specifies the flow name. It can be one of: 显示在两端的数据流。输出与从GUI保存的ASCII格式相同。 12345any All framesicmp ICMPicmpv6 ICMPv6lbm_uim UIMtcp TCP mode specifies the address type. It can be one of: mode 指定了地址类型。可以是以下之一 12standard Any addressnetwork Network address Example: -z flow,tcp,network will show data flow for all TCP frame 例如： -z flow,tcp,network将展示所有TCP帧的数据流 -z follow,prot,mode,filter[,range] Displays the contents of a TCP or UDP stream between two nodes. 展示在两端的TCP或是UDP流的内容。 prot specifies the transport protocol. It can be one of: 123tcp TCPudp UDPtls TLS or SSL mode specifies the output mode. It can be one of: 1234ascii ASCII output with dots for non-printable charactersebcdic EBCDIC output with dots for non-printable charactershex Hexadecimal and ASCII data with offsetsraw Hexadecimal data filter specifies the stream to be displayed. UDP/TCP streams are selected with either the stream index or IP address plus port pairs. TLS streams are selected with the stream index. For example: 12ip-addr0:port0,ip-addr1:port1stream-index range optionally specifies which “chunks” of the stream should be displayed. Example: -z “follow,tcp,hex,1” will display the contents of the second TCP stream (the first is stream 0) in “hex” format. 例如： -z “follow,tcp,hex,1”将会使用“hex”格式展示第二个TCP流的内容（第一个是stream 0） 1234567891011&gt; ===================================================================&gt; Follow: tcp,hex&gt; Filter: tcp.stream eq 1&gt; Node 0: 200.57.7.197:32891&gt; Node 1: 200.57.7.198:2906&gt; 00000000 00 00 00 22 00 00 00 07 00 0a 85 02 07 e9 00 02 ...&quot;.... ........&gt; 00000010 07 e9 06 0f 00 0d 00 04 00 00 00 01 00 03 00 06 ........ ........&gt; 00000020 1f 00 06 04 00 00 ......&gt; 00000000 00 01 00 00 ....&gt; 00000026 00 02 00 00&gt; Example: -z “follow,tcp,ascii,200.57.7.197:32891,200.57.7.198:2906” will display the contents of a TCP stream between 200.57.7.197 port 32891 and 200.57.7.98 port 2906. 例如：-z “follow,tcp,ascii,200.57.7.197:32891,200.57.7.198:2906”将会展示200.57.7.197的32891端口与200.57.7.98的2906端口的TCP流的内容 1234567891011&gt; ===================================================================&gt; Follow: tcp,ascii&gt; Filter: (omitted for readability)&gt; Node 0: 200.57.7.197:32891&gt; Node 1: 200.57.7.198:2906&gt; 38&gt; ...&quot;.....&gt; ................&gt; 4&gt; ....&gt; -z hosts [,ipv4][,ipv6] Dump any collected IPv4 and/or IPv6 addresses in “hosts” format. Both IPv4 and IPv6 addresses are dumped by default.Addresses are collected from a number of sources, including standard “hosts” files and captured traffic. -z http,stat, Calculate the HTTP statistics distribution. Displayed values are the HTTP status codes and the HTTP request methods. -z http,tree Calculate the HTTP packet distribution. Displayed values are the HTTP request modes and the HTTP status codes. -z http_srv,tree Calculate the HTTP requests and responses by server. For the HTTP requests, displayed values are the server IP address and server hostname. For the HTTP responses, displayed values are the server IP address and status. -z icmp,srt[,filter] Compute total ICMP echo requests, replies, loss, and percent loss, as well as minimum, maximum, mean, median and sample standard deviation SRT statistics typical of what ping provides. Example: -z icmp,srt,ip.src==1.2.3.4 will collect ICMP SRT statistics for ICMP echo request packets originating from a specific host.This option can be used multiple times on the command line. 计算总的ICMP回应请求，回复，丢失和百分比损失，以及ping提供的典型的最小值，最大值，平均值，中值和样本标准差SRT统计量。 例如：-z icmp,srt,ip.src==1.2.3.4将收集源自特定主机的ICMP回送请求数据包的ICMP SRT统计信息。可以在命令行上多次使用此选项。 -z io,phs[,filter] Create Protocol Hierarchy Statistics listing both number of packets and bytes. If no filter is specified the statistics will be calculated for all packets. If a filter is specified statistics will only be calculated for those packets that match the filter.This option can be used multiple times on the command line. 创建一个包含所有数据包和字节数的协议分层信息列表。 -z io,stat,interval[,filter][,filter][,filter]… Collect packet/bytes statistics for the capture in intervals of interval seconds. Interval can be specified either as a whole or fractional second and can be specified with microsecond (us) resolution. If interval is 0, the statistics will be calculated over all packets. 按时间间隔秒数收集捕获的数据包/字节数统计信息。时间间隔可以指定为整数或小数秒，并且可以使用微秒（us）。如果interval为0，则将计算所有数据包的统计信息。 If no filter is specified the statistics will be calculated for all packets.If one or more filters are specified statistics will be calculated for all filters and presented with one column of statistics for each filter. This option can be used multiple times on the command line. 如果没有指定过滤条件，则计算所有数据包的统计信息。如果一个或多个过滤条件被指定，那么会为所有的过滤条件来计算统计信息，并为每个过滤条件显示一列统计信息。可以在命令行上多次使用此选项。 Example: -z io,stat,1,ip.addr==1.2.3.4 will generate 1 second statistics for all traffic to/from host 1.2.3.4. 例如： -z io,stat,1,ip.addr==1.2.3.4 将为所有进出主机1.2.3.4的流量生成1秒的统计信息 Example: -z “io,stat,0.001,smb&amp;&amp;ip.addr==1.2.3.4” will generate 1ms statistics for all SMB packets to/from host 1.2.3.4. 例如： -z io,stat,1,smb&amp;&amp;ip.addr==1.2.3.4 将为所有进出主机1.2.3.4的SMB流量生成1毫秒的统计信息 The examples above all use the standard syntax for generating statistics which only calculates the number of packets and bytes in each interval. 上面的示例都使用标准语法来生成统计信息，该统计信息仅计算每个间隔中的数据包和字节数。 -z io,stat,interval,”[COUNT|SUM|MIN|MAX|AVG|LOAD](field)filter“ io,stat can also do much more statistics and calculate COUNT(), SUM(), MIN(), MAX(), AVG() and LOAD() using a slightly different filter syntax: io,stat还能做更多的统计和计算。如COUNT(), SUM(), MIN(), MAX(), AVG() and LOAD()，这些使用稍微不同的过滤器语法。]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用github+hexo创建一个博客]]></title>
    <url>%2F2019%2F04%2F25%2F%E4%BD%BF%E7%94%A8github-hexo%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[使用github+hexo创建一个博客github建库过程略 在Windows系统下，要使用hexo需要先安装Nodejs以及Git 1.安装NodejsNodejs下载地址，下载之后，一路默认安装 2.安装GitGit下载地址，下载之后又是一路安装 3.安装Hexo3.1创建一个文件夹创建一个文件夹，比如hexo。来存放数据(这个blog不用管，是后面初始化后自动创建的) 在这个目录下右键，选择红框所示内容。之后会弹出一个框，里面使用的是bash命令。 3.2安装过程在界面中输入下面内容，关于指令含义参考：Hexo-指令 1234567$ cd d:/hexo$ npm install hexo-cli -g $ hexo init blog #初始化一个文件夹，命名为blog$ cd blog$ npm install$ hexo g # 或者hexo generate$ hexo s # 或者hexo server，可以在http://localhost:4000/ 查看 经常使用的命令： hexo generate (hexo g) 生成静态文件，会在当前目录下生成一个新的叫做public的文件夹 hexo server (hexo s) 启动本地web服务，用于博客的预览 hexo deploy (hexo d) 部署播客到远端（比如github, heroku等平台） hexo new “postName” 新建文章，postname指创建的文件名，文件会放在source\ _posts\下 hexo new page “pageName” 新建页面，pagename指创建的网页文件夹名，文件夹放在source\下 3.3更换主题123$ hexo clean #清除缓存文件 (db.json) 和已生成的静态文件 (public)。 #在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需 #要运行该命令$ git clone https://github.com/zhwangart/hexo-theme-ocean.git themes/ocean 3.3.1使用主题修改Hexo目录下的_config.yml配置文件中的theme属性，将其设置为ocean。 3.3.2更新主题12345$ cd themes/ocean$ git pull$ cd ..$ hexo g # 生成$ hexo s # 启动本地web服务器 4.使用hexo deploy部署部署命令说明 4.1扩展安装1npm install hexo-deployer-git --save 4.2修改配置 1234deploy: type:git repo:https://github.com/dilidonglong/dilidonglong.github.io.git branch:master 4.3完成部署然后在命令行中执行 1$ hexo d #部署之前预先生成静态文件 即可完成部署。 4.4其他注意事项上述命令虽然简单方便，但是偶尔会有莫名其妙的问题出现，因此，我们也可以追本溯源，使用git命令来完成部署的工作。 12$ cd d:/hexo/blog$ git clone https://github.com/dilidonglong/dilidonglong.github.io.git .deploy/dilidonglong.github.io 将我们之前创建的仓库克隆到本地，新建一个目录叫做.deploy用于存放克隆的代码。前提是这个库里面有你之前上传上去的内容。 接下来创建一个deploy脚本文件，比如可以取名叫deploy.sh 1234567hexo generatecp -R public/* .deploy/dilidonglong.github.iocd .deploy/dilidonglong.github.iogit add .git commit -m “update”git pull --rebase origin mastergit push origin master 简单解释一下，hexo generate（hexo g）生成public文件夹下的新内容，然后将其拷贝至dilidonglong.github.io的git目录下，然后使用git commit命令提交代码到dilidonglong.github.io这个repo的master branch上。 需要部署的时候，执行这段脚本就可以了（比如可以将其保存为deploy.sh）。执行过程中可能需要让你输入Github账户的用户名及密码，按照提示操作即可。执行时，等同linux命令行：./deploy.sh 12git pull --rebase origin master对于这条命令，是为了解决git push错误failed to push some refs to的问题 上述问题的参考链接 参考文章： 手把手教你使用Hexo + Github Pages搭建个人独立博客]]></content>
      <categories>
        <category>小技巧</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[刚认识东东的时候。我说：“朋友有叫我昭阳的，有叫我西西的。你怎么称呼？” ——“西～西~？我叫东东，dilidonglong的东东” 新浪微博：望伊如西]]></content>
  </entry>
  <entry>
    <title><![CDATA[分类]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[归档]]></title>
    <url>%2Farchives%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[相册]]></title>
    <url>%2Fgallery%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
